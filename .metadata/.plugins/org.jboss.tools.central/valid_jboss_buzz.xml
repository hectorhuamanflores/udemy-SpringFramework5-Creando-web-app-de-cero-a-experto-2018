<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>This week in JBoss (25th May 2018): Updates from Summit and more</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/jUts19CsDqU/this-week-in-jboss-25th-may-2018-updates-from-summit-and-more" /><category term="apache camel" scheme="searchisko:content:tags" /><category term="debezium" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_weeklyeditorial" scheme="searchisko:content:tags" /><category term="Hibernate" scheme="searchisko:content:tags" /><category term="jbpm 7" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="narayana" scheme="searchisko:content:tags" /><category term="news" scheme="searchisko:content:tags" /><category term="red hat summit" scheme="searchisko:content:tags" /><category term="Teiid" scheme="searchisko:content:tags" /><category term="weekly_editorial" scheme="searchisko:content:tags" /><category term="weekly_udpate" scheme="searchisko:content:tags" /><author><name>Kevin Conner</name></author><id>searchisko:content:id:jbossorg_blog-this_week_in_jboss_25th_may_2018_updates_from_summit_and_more</id><updated>2018-05-26T00:04:40Z</updated><published>2018-05-26T00:03:32Z</published><content type="html">&lt;!-- [DocumentBodyStart:cceca6e7-72e8-4cbe-bd96-9e81cff936ae] --&gt;&lt;div class="jive-rendered-content"&gt;&lt;p&gt;Welcome to another edition of the JBoss Editorial, our regular search through the JBoss Communities in search of interesting topic for all.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Updates from Red Hat Summit&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Earlier this month saw the largest &lt;a class="jive-link-external-small" href="https://www.redhat.com/en/summit/2018" rel="nofollow"&gt;Red Hat Summit conference&lt;/a&gt; taking place in San Francisco's Moscone Convention Center, if you missed the conference or would like a recap then we have some good articles for you to read.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;The first article of the week covers a presentation given by Marius Bogoevici and Christian Posta and entitled "An Eventful Tour from Enterprise Integration to Serverless", taking the audience on a &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2018/05/18/eventful-tour-enterprise-integration-to-serverless/" rel="nofollow"&gt;tour covering the basic of Enterprise Integration through the microservices and serverless computing&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;The second article covers a presentation given by Christian Posta entitled "Lowering the risk of monolith to microservices", &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2018/05/21/red-hat-summit-lowering-the-risk-of-monolith-to-microservices/" rel="nofollow"&gt;discussing the journey of a fictitious company as they consider a pragmatic approach for moving their monolithic application over to a microservices architecture&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;The third article takes us behind the scenes &lt;a class="jive-link-external-small" href="https://www.optaplanner.org/blog/2018/05/23/BehindTheScenesOfRedHatSummitScheduling.html" rel="nofollow"&gt;for a look at how sessions at such a large conference are scheduled and the part OptaPlanner played in optimising the agenda&lt;/a&gt;.&amp;#160; In the article Geoffrey covers not only the theoretical challenge to the task but also the practical challenges which arose as they worked towards the generation of the agenda.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Would you like to &lt;a class="jive-link-external-small" href="http://www.schabell.org/2018/05/get-a-signed-copy-of-effective-business-process-management-with-jboss-bpm.html" rel="nofollow"&gt;get a signed copy of the book "Effective Business Process Management with JBoss BPM"?&lt;/a&gt;&amp;#160; The author of the book, Eric Schabell, was signing and giving away copies at Summit and still has a few copies left.&amp;#160; Getting hold of a copy is easy, add a comment on the article sharing your JBoss BPM story and let Eric know you have done so via twitter.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Querying Debezium Change Data Events With KSQL&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;With Debezium's ability to turn database updates into an event stream, publishing the stream through Kafka Connect and allowing applications to respond near instantaneously to each committed change, we have a very powerful mechanism for monitoring and reacting to database changes and when coupled with &lt;a class="jive-link-external-small" href="https://github.com/confluentinc/ksql" rel="nofollow"&gt;KSQL&lt;/a&gt;, a streaming SQL engine build on &lt;a class="jive-link-external-small" href="https://kafka.apache.org/documentation/streams/" rel="nofollow"&gt;Kafka Streams&lt;/a&gt;, it enables the ability to interactively process the Change Data Events as they arise.&amp;#160; &lt;a class="jive-link-external-small" href="http://debezium.io/blog/2018/05/24/querying-debezium-change-data-eEvents-with-ksql/" rel="nofollow"&gt;For more information on this topic, as well as a demo showing how to start a KSQL instance, map KSQL streams to Debezium topics and repartioning of data, checkout Jiri's post&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;&lt;span style="color: #3d3d3d;"&gt;Using the Contract Net Protocol in jBPM&lt;/span&gt;&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;The &lt;a class="jive-link-external-small" href="https://en.wikipedia.org/wiki/Contract_Net_Protocol" rel="nofollow"&gt;Contract Net Protocol&lt;/a&gt; allows multiple agents to announce and bid for contracts in order to complete an item of work, coordinating the execution of the work through an announcement/bidding/awarding process and can be &lt;a class="jive-link-external-small" href="http://mswiderski.blogspot.ca/2018/05/contract-net-protocol-with-jbpm.html" rel="nofollow"&gt;modelled in jPBM through case definitions where individual phases of the protocol can be externalised via processes&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Apache Camel Language Support for Eclipse, VS Code and OpenShift.io&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;The Camel Language Server, enabling completion and documentation for camel URIs, is now available for download through the Eclipse and VS Code marketplaces as well as being available in OpenShift.io.&amp;#160; If you work with Camel on these platforms then take a look at Aur&amp;eacute;lien's article where &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2018/05/21/apache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io/" rel="nofollow"&gt;he explains how to download and enable the plugins within each environment&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Hibernate Community Newsletter&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;The latest version of the &lt;a class="jive-link-external-small" href="http://in.relation.to/2018/05/25/hibernate-community-newsletter-2018-10/" rel="nofollow"&gt;Hibernate Community Newsletter&lt;/a&gt; is out with new articles from the Hibernate Community.&amp;#160; In this edition of the newsletter you will find articles explaining how JPA and hibernate can simplify data persistence, how to bootstrap JPA and Hibernate within Java SE, a beginner&amp;#8217;s guide to Linearizability, mapping a ZonedDateTime, the Dirty Read phenomenon and many more.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Narayana JDBC Integration for Tomcat&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;If you are interested in using JTA capabilities within your tomcat deployment then take a look at Ondra's article &lt;a class="jive-link-external-small" href="http://jbossts.blogspot.ca/2018/05/narayana-jdbc-integration-for-tomcat.html" rel="nofollow"&gt;discussing the integration of Narayana within tomcat&lt;/a&gt;.&amp;#160; Ondra discusses three different approaches to integration, examining the setup for the Narayana JDBC transactional driver, the Apache Commons DBCP2 library and lastly IronJacamar JCA.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;JBoss Out and About&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Eric Schabell will be &lt;a class="jive-link-external-small" href="http://www.schabell.org/2018/05/scotland-jbug-appdev-in-the-cloud-workshop.html" rel="nofollow"&gt;visiting Edinburgh, Scotland on June 6th to give a workshop entitled "AppDev in the Cloud"&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;New Releases&lt;/h2&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The &lt;a class="jive-link-external-small" href="http://teiid.org/" rel="nofollow"&gt;Teiid&lt;/a&gt; team have announced the release of &lt;a class="jive-link-external-small" href="http://teiid.blogspot.ca/2018/05/teiid-1014-released.html" rel="nofollow"&gt;Teiid 10.1.4&lt;/a&gt;, &lt;a class="jive-link-external-small" href="http://teiid.blogspot.ca/2018/05/teiid-1022-released.html" rel="nofollow"&gt;Teiid 10.2.2&lt;/a&gt; and &lt;a class="jive-link-external-small" href="http://teiid.blogspot.ca/2018/05/teiid-1031-released.html" rel="nofollow"&gt;Teiid 10.3.1&lt;/a&gt;&lt;/li&gt;&lt;li&gt;The &lt;a class="jive-link-external-small" href="http://hibernate.org/" rel="nofollow"&gt;Hibernate&lt;/a&gt; team have announced the release of &lt;a class="jive-link-external-small" href="http://in.relation.to/2018/05/25/hibernate-ogm-5-4-Beta1-released/" rel="nofollow"&gt;Hibernate OGM 5.4.0.Beta1&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;That's all from this week's editorial, please join us again next week when we will take another spin through the JBoss Communities in search of interesting articles.&lt;/p&gt;&lt;/div&gt;&lt;!-- [DocumentBodyEnd:cceca6e7-72e8-4cbe-bd96-9e81cff936ae] --&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/jUts19CsDqU" height="1" width="1" alt=""/&gt;</content><summary>Welcome to another edition of the JBoss Editorial, our regular search through the JBoss Communities in search of interesting topic for all.   Updates from Red Hat Summit   Earlier this month saw the largest Red Hat Summit conference taking place in San Francisco's Moscone Convention Center, if you missed the conference or would like a recap then we have some good articles for you to read.   The fi...</summary><dc:creator>Kevin Conner</dc:creator><dc:date>2018-05-26T00:03:32Z</dc:date><feedburner:origLink>https://developer.jboss.org/blogs/weekly-editorial/2018/05/25/this-week-in-jboss-25th-may-2018-updates-from-summit-and-more</feedburner:origLink></entry><entry><title>Simplify Local Variable Type Definition Using the Java 10 var Keyword</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ppecfk__NEE/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="java 10" scheme="searchisko:content:tags" /><category term="local variable type inference" scheme="searchisko:content:tags" /><category term="type definition" scheme="searchisko:content:tags" /><category term="var keyword" scheme="searchisko:content:tags" /><author><name>unknown</name></author><id>searchisko:content:id:jbossorg_blog-simplify_local_variable_type_definition_using_the_java_10_var_keyword</id><updated>2018-05-25T11:00:38Z</updated><published>2018-05-25T11:00:38Z</published><content type="html">&lt;p&gt;As many of you might have heard, &lt;a href="http://www.oracle.com/technetwork/java/javase/10-relnote-issues-4108729.html"&gt;Java 10&lt;/a&gt; was released in March 2018. It is a short-term release from Oracle Corporation and came with lot of new features and enhancements. One of the important features in Java 10 is l&lt;em&gt;ocal variable type inference&lt;/em&gt;, which is detailed in &lt;a href="http://openjdk.java.net/jeps/286"&gt;JEP (Java Enhancement Proposal) 286&lt;/a&gt;. The upcoming Java release, due in September 2018, will be a long-term-support (LTS) version of Java. (Note that generally, LTS releases are due every three years.)&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s look at a  Java 10 &lt;em&gt;local variable type inference&lt;/em&gt; feature example now.&lt;/p&gt; &lt;p&gt;&lt;span id="more-496217"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;The main advantage of this feature is to reduce boilerplate variable type definitions and to increase code readability. Here&amp;#8217;s an example:&lt;/p&gt; &lt;pre&gt;String s=new String("Java 10"); Integer int=new Integer(10); &lt;/pre&gt; &lt;p&gt;A Java developer would have no problem reading the above two statements. However, as another example, here are some more-complex statements that are kind of pain to write:&lt;/p&gt; &lt;pre&gt;MAP&amp;#60;String,String&amp;#62; map=new HashMap&amp;#60;String,String&amp;#62;(); MAP&amp;#60;User,List&amp;#60;String&amp;#62;&amp;#62; listofMovies=new HashMap&amp;#60;&amp;#62;(); &lt;/pre&gt; &lt;p&gt;In Java 10, the &lt;code&gt;var&lt;/code&gt; keyword allows local variable type inference, which means the type for the local variable will be inferred by the compiler, so you don&amp;#8217;t need to declare that. Hence, you can replace the above two statements as shown below:&lt;/p&gt; &lt;pre&gt;var map=new HashMap&amp;#60;String,String&amp;#62;(); var listofMovies=new HashMap&amp;#60;User,List&amp;#60;String&amp;#62;&amp;#62;();&lt;/pre&gt; &lt;p&gt;Below are the few points to remember about local variable type inference in Java 10:&lt;/p&gt; &lt;p&gt;1. Each statement containing the &lt;code&gt;var&lt;/code&gt; keyword has a static type which is the declared type of value. This means that assigning a value of a different type will always fail. Hence, Java is still a statically typed language (unlike JavaScript), and there should be enough information to infer the type of a local variable. If that is not there, compilation fails, for example:&lt;/p&gt; &lt;pre&gt;var id=0;// At this moment, compiler interprets //variable id as integer. id="34"; // This will result in compilation error //because of incompatible types: java.lang.String //can't be converted to int. &lt;/pre&gt; &lt;p&gt;Notice that JavaScript also has the concept of a &lt;code&gt;var&lt;/code&gt; keyword, but that is completely different from Java 10 &lt;code&gt;var&lt;/code&gt;. JavaScript does not have type definitions for variables. As a result, the above example would have been successfully interpreted by the JavaScript runtime, and that is one of the reasons TypeScript was introduced.&lt;/p&gt; &lt;p&gt;2. Let&amp;#8217;s look at an inheritance scenario. Assume there are two subclasses (&lt;code&gt;Doctor&lt;/code&gt;, &lt;code&gt;Engineer&lt;/code&gt;) extended from the parent class &lt;code&gt;Person&lt;/code&gt;. Let&amp;#8217;s say someone creates an object of &lt;code&gt;Doctor&lt;/code&gt;, as shown below:&lt;/p&gt; &lt;pre&gt;var p=new Doctor(); // In this case, what should be //the type of p; it is Doctor or Person?&lt;/pre&gt; &lt;p&gt;Note that in such cases, a variable declared with &lt;code&gt;var&lt;/code&gt; is always the type of the initializer (&lt;code&gt;Doctor&lt;/code&gt;, in this case), and &lt;code&gt;var&lt;/code&gt; may not be used when there is no initializer. Therefore, if you reassign the above variable &lt;code&gt;p&lt;/code&gt;, as shown below, compilation fails:&lt;/p&gt; &lt;pre&gt;p=new Engineer(); // Compilation error saying //incompatible types &lt;/pre&gt; &lt;p&gt;So we can say that &lt;em&gt;polymorphic behavior does not work with the &lt;/em&gt;&lt;code&gt;var&lt;/code&gt;&lt;em&gt; keyword&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;3. The following  are places where you cannot use local variable type inference:&lt;/p&gt; &lt;p&gt;a)  You can&amp;#8217;t use local variable type inference with method arguments:&lt;/p&gt; &lt;pre&gt;public long countNumberofFiles(var fileList);// Compilation //error because compiler cannot infer type of local //variable fileList; cannot use 'var' on variable without //initializer &lt;/pre&gt; &lt;p&gt;b) You cannot initialize a &lt;code&gt;var&lt;/code&gt; variable to null. By assigning null, it is not clear what the type should be, since in Java, any object reference can be null. In the following example, because there is no predefined data type for a null value, the compiler is not able to interpret a type for &lt;code&gt;count&lt;/code&gt;, which would cause a complication error.&lt;/p&gt; &lt;pre&gt;var count=null;// Compilation error because //compiler cannot infer type for local variable //count since any Java object reference can be null &lt;/pre&gt; &lt;p&gt;Note that JavaScript has a data type NULL, which can hold only one value: Null.&lt;/p&gt; &lt;p&gt;c) You can&amp;#8217;t use local variable type inference with lambda expressions, because those require an explicit target type. For example, the following causes a compilation error:&lt;/p&gt; &lt;pre&gt;var z = () -&amp;#62; {} // Compilation error because //compiler cannot infer type for local variable z; //lambda expression needs an explicit target type &lt;/pre&gt; &lt;p&gt;Java 10 &lt;code&gt;var&lt;/code&gt; is desgined to improve the readability of code for other developers who read the  code. In some situations, it can be good to use &lt;code&gt;var&lt;/code&gt;; however, in other situations it can reduce the readability of code.&lt;/p&gt; &lt;p&gt;Here&amp;#8217;s an example of looping over an &lt;code&gt;entrySet&lt;/code&gt; of a &lt;code&gt;Map&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;Map&amp;#60;String, List&amp;#60;String&amp;#62;&amp;#62; companyToEmployees= new HashMap&amp;#60;&amp;#62;(); for (Map.Entry&amp;#60;String, List&amp;#60;String&amp;#62;&amp;#62; entry: companyToEmployees . entrySet()) {       List&amp;#60;String&amp;#62; employees= entry.getValue(); } &lt;/pre&gt; &lt;p&gt;Let&amp;#8217;s rewrite the above code using Java 10 &lt;code&gt;var&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;var companyToEmployees= new HashMap&amp;#60;String, List&amp;#60;String&amp;#62;&amp;#62;(); for (var entry: companyToEmployees. entrySet()) {       var employees= entry.getValue(); } &lt;/pre&gt; &lt;p&gt;From the above example, it is clear that using &lt;code&gt;var&lt;/code&gt; might not always be good.&lt;/p&gt; &lt;p&gt;For more information about the &lt;code&gt;var&lt;/code&gt; keyword, I recommend going through the &lt;a href="http://openjdk.java.net/jeps/286"&gt;Java 10 local variable type reference docs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F25%2Fsimplify-local-variable-type-definition-using-the-java-10-var-keyword%2F&amp;#38;linkname=Simplify%20Local%20Variable%20Type%20Definition%20Using%20the%20Java%2010%20var%20Keyword" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F25%2Fsimplify-local-variable-type-definition-using-the-java-10-var-keyword%2F&amp;#38;linkname=Simplify%20Local%20Variable%20Type%20Definition%20Using%20the%20Java%2010%20var%20Keyword" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F25%2Fsimplify-local-variable-type-definition-using-the-java-10-var-keyword%2F&amp;#38;linkname=Simplify%20Local%20Variable%20Type%20Definition%20Using%20the%20Java%2010%20var%20Keyword" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F25%2Fsimplify-local-variable-type-definition-using-the-java-10-var-keyword%2F&amp;#38;linkname=Simplify%20Local%20Variable%20Type%20Definition%20Using%20the%20Java%2010%20var%20Keyword" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F25%2Fsimplify-local-variable-type-definition-using-the-java-10-var-keyword%2F&amp;#38;linkname=Simplify%20Local%20Variable%20Type%20Definition%20Using%20the%20Java%2010%20var%20Keyword" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F25%2Fsimplify-local-variable-type-definition-using-the-java-10-var-keyword%2F&amp;#38;linkname=Simplify%20Local%20Variable%20Type%20Definition%20Using%20the%20Java%2010%20var%20Keyword" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F25%2Fsimplify-local-variable-type-definition-using-the-java-10-var-keyword%2F&amp;#38;linkname=Simplify%20Local%20Variable%20Type%20Definition%20Using%20the%20Java%2010%20var%20Keyword" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F25%2Fsimplify-local-variable-type-definition-using-the-java-10-var-keyword%2F&amp;#38;linkname=Simplify%20Local%20Variable%20Type%20Definition%20Using%20the%20Java%2010%20var%20Keyword" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F25%2Fsimplify-local-variable-type-definition-using-the-java-10-var-keyword%2F&amp;#38;title=Simplify%20Local%20Variable%20Type%20Definition%20Using%20the%20Java%2010%20var%20Keyword" data-a2a-url="https://developers.redhat.com/blog/2018/05/25/simplify-local-variable-type-definition-using-the-java-10-var-keyword/" data-a2a-title="Simplify Local Variable Type Definition Using the Java 10 var Keyword"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/25/simplify-local-variable-type-definition-using-the-java-10-var-keyword/"&gt;Simplify Local Variable Type Definition Using the Java 10 var Keyword&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ppecfk__NEE" height="1" width="1" alt=""/&gt;</content><summary>As many of you might have heard, Java 10 was released in March 2018. It is a short-term release from Oracle Corporation and came with lot of new features and enhancements. One of the important features in Java 10 is local variable type inference, which is detailed in JEP (Java Enhancement Proposal) 286. The upcoming Java release, due in September 2018, will be a long-term-support (LTS) version of ...</summary><dc:creator>unknown</dc:creator><dc:date>2018-05-25T11:00:38Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/25/simplify-local-variable-type-definition-using-the-java-10-var-keyword/</feedburner:origLink></entry><entry><title>Hibernate OGM 5.4.0.Beta1 release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ZJxIufc6c7c/" /><category term="feed_group_name_hibernate" scheme="searchisko:content:tags" /><category term="feed_name_inrelationto" scheme="searchisko:content:tags" /><category term="hibernate ogm" scheme="searchisko:content:tags" /><category term="releases" scheme="searchisko:content:tags" /><author><name>unknown</name></author><id>searchisko:content:id:jbossorg_blog-hibernate_ogm_5_4_0_beta1_release</id><updated>2018-05-25T10:21:49Z</updated><published>2018-05-25T00:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;&lt;a href="http://hibernate.org/ogm/releases/5.4/#get-it"&gt;Hibernate OGM 5.4.0.Beta1&lt;/a&gt; has been released!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The main differences from the previous release is the upgrade to Hibernate ORM 5.3.0.Final, the support for Infinispan Remote native and JPQL queries (without requiring Hibernate Search), and the use of cluster counters for local caches when generating sequences with Infinispan Embedded.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;More detail available in the &lt;a href="https://hibernate.atlassian.net/secure/ReleaseNote.jspa?projectId=10160&amp;amp;version=31660"&gt;release notes&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="hibernate-orm-5-3"&gt;&lt;a class="anchor" href="#hibernate-orm-5-3"&gt;&lt;/a&gt;Hibernate ORM 5.3&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Hibernate ORM 5.3 has a huge list of improvements, most notably it implements JPA 2.2.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can find all the details about it in the &lt;a href="http://in.relation.to/2018/05/14/hibernate-orm-530-final-release/"&gt;Hibernate ORM 5.3.0.Final release post&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="infinispan-remote-queries"&gt;&lt;a class="anchor" href="#infinispan-remote-queries"&gt;&lt;/a&gt;Infinispan Remote queries&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can execute queries in Infinispan using the query language called &lt;a href="https://blog.infinispan.org/2016/12/meet-ickle.html"&gt;Ickle&lt;/a&gt;. This latest version of Hibernate OGM will convert the JPQL queries into an equivalent in Ickle. You can also run native query directly.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Here is an example of a native query execution:&lt;/p&gt; &lt;/div&gt; &lt;div class="exampleblock"&gt; &lt;div class="content"&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="JAVA"&gt;&lt;span style="color:#0a8;font-weight:bold"&gt;String&lt;/span&gt; ickleQuery = &lt;span style="background-color:hsla(0,100%,50%,0.05)"&gt;&lt;span style="color:#710"&gt;"&lt;/span&gt;&lt;span style="color:#D20"&gt;from HibernateOGMGenerated.Registry where level &amp;gt; 3 order by start&lt;/span&gt;&lt;span style="color:#710"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span style="color:#0a8;font-weight:bold"&gt;List&lt;/span&gt; result = session.createNativeQuery( ickleQuery ) .addEntity( Employee.class ) .list();&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;With Ickle you can also write queries which use Infinispan’s full-text capabilities backed by Apache Lucene, as long as the field is indexed. It’s a nice language combining some notions from JPQL and the Lucene query languages.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Note that this will only work if the cache already exists on the server and the fields are indexed. Currently Hibernate OGM is able to automatically generate protobuf schemas but the generated metadata will not include index definitions, so you might need to add these as needed.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="where-can-i-get-hibernate-ogm"&gt;&lt;a class="anchor" href="#where-can-i-get-hibernate-ogm"&gt;&lt;/a&gt;Where can I get Hibernate OGM?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can include in your project the dialect of your choice using these Maven coordinates:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://infinispan.org"&gt;Infinispan&lt;/a&gt;&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Remote: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-infinispan-remote:5.4.0.Beta1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Embedded: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-infinispan-embedded:5.4.0.Beta1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.mongodb.com"&gt;MongoDB&lt;/a&gt;: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-mongodb:5.4.0.Beta1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://neo4j.com"&gt;Neo4j&lt;/a&gt;: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-neo4j:5.4.0.Beta1&lt;/em&gt;&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Infinispan Remote: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-featurepack-infinispan-remote:5.4.0.Beta1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Infinispan Embedded: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-featurepack-infinispan-embedded:5.4.0.Beta1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;MongoDB: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-featurepack-mongodb:5.4.0.Beta1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Neo4j: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-featurepack-neo4j:5.4.0.Beta1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Alternatively, you can download archives containing all the binaries, source code and documentation &lt;a href="https://sourceforge.net/projects/hibernate/files/hibernate-ogm/5.4.0.Beta1"&gt;from Sourceforge&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If you are interested about available versions, you can check the official &lt;a href="http://hibernate.org/ogm/releases"&gt;Hibernate OGM download page&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="how-can-i-get-in-touch"&gt;&lt;a class="anchor" href="#how-can-i-get-in-touch"&gt;&lt;/a&gt;How can I get in touch?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can find us through the following channels:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/c/hibernate-ogm"&gt;User forum&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://hibernate.atlassian.net/browse/OGM"&gt;Issue tracker&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://lists.jboss.org/pipermail/hibernate-dev/"&gt;Mailing list&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://stackoverflow.com"&gt;Stack Overflow&lt;/a&gt;: we monitor the tag &lt;em&gt;hibernate-ogm&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.hipchat.com/gXEjW5Wgg"&gt;HipChat&lt;/a&gt;: Hibernate OGM hipchat room&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="contributions"&gt;&lt;a class="anchor" href="#contributions"&gt;&lt;/a&gt;Contributions&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Some of the new features have been contributed by Bato-Bair, Sergey Chernolyas and The Viet Nguyen. Thanks a lot!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We are looking forward to hearing your feedback!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ZJxIufc6c7c" height="1" width="1" alt=""/&gt;</content><summary>Hibernate OGM 5.4.0.Beta1 has been released! The main differences from the previous release is the upgrade to Hibernate ORM 5.3.0.Final, the support for Infinispan Remote native and JPQL queries (without requiring Hibernate Search), and the use of cluster counters for local caches when generating sequences with Infinispan Embedded. More detail available in the release notes. Hibernate ORM 5.3 Hibe...</summary><dc:creator>unknown</dc:creator><dc:date>2018-05-25T00:00:00Z</dc:date><feedburner:origLink>http://in.relation.to/2018/05/25/hibernate-ogm-5-4-Beta1-released/</feedburner:origLink></entry><entry><title>Detecting String Truncation with GCC 8</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/jecJ3uw8Cdk/" /><category term="C" scheme="searchisko:content:tags" /><category term="C++" scheme="searchisko:content:tags" /><category term="community" scheme="searchisko:content:tags" /><category term="compiler" scheme="searchisko:content:tags" /><category term="Developer Tools" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="gcc" scheme="searchisko:content:tags" /><category term="GCC 8" scheme="searchisko:content:tags" /><category term="GCC compiler collection" scheme="searchisko:content:tags" /><category term="GNU Compiler Collection" scheme="searchisko:content:tags" /><category term="memory" scheme="searchisko:content:tags" /><category term="Programming Languages" scheme="searchisko:content:tags" /><category term="security" scheme="searchisko:content:tags" /><category term="string truncation" scheme="searchisko:content:tags" /><author><name>unknown</name></author><id>searchisko:content:id:jbossorg_blog-detecting_string_truncation_with_gcc_8</id><updated>2018-05-24T10:55:15Z</updated><published>2018-05-24T10:55:15Z</published><content type="html">&lt;p&gt;Continuing in the effort to detect common programming errors, the just-released GCC 8 contains a number of new warnings as well as enhancements to existing checkers to help find non-obvious bugs in C and C++ code. This article focuses on those that deal with inadvertent string truncation and discusses some of the approaches for avoiding the underlying problems. If you haven&amp;#8217;t read it, you might also want to read David Malcolm&amp;#8217;s article &lt;a href="https://developers.redhat.com/blog/2018/03/15/gcc-8-usability-improvements/"&gt;&lt;em&gt;Usability improvements in GCC 8&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Why Is String Truncation a Problem?&lt;/h2&gt; &lt;p&gt;It is well-known why buffer overflow is dangerous: writing past the end of an object can overwrite data in adjacent storage, resulting in data corruption. In the most benign cases, the corruption can simply lead to incorrect behavior of the program. If the adjacent data is an address in the executable text segment, the corruption may be exploitable to gain control of the affected process, which can lead to a security vulnerability. (See &lt;a title="Improper Restriction of Operations within the Bounds of a Memory Buffer" href="https://cwe.mitre.org/data/definitions/119.html"&gt;CWE-119&lt;/a&gt; for more on buffer overflow.)&lt;/p&gt; &lt;p&gt;&lt;span id="more-496717"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;But string truncation does not overwrite any data, so why is it a problem? Inadvertently truncating a string can be considered data corruption: it is the creation of a sequence of characters from which some of the trailing characters are unintentionally missing. String truncation can take one of two general forms. One results in a NUL-terminated string that is shorter than the sum of the lengths of the concatenated strings. The other results in a sequence of bytes not terminated by a NUL character: that is, the result is not a string. Using such a result where a string is expected is undefined. (See &lt;a title="Improper Null Termination" href="https://cwe.mitre.org/data/definitions/170.html"&gt;CWE-170&lt;/a&gt; for more about weaknesses resulting from improper string termination.) The different kinds of truncation are caused by different functions and their detection is controlled by different warning options, both of which are enabled by &lt;code&gt;&lt;tt&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-8.1.0/gcc/Warning-Options.html#index-Wall"&gt;-Wall&lt;/a&gt;&lt;/tt&gt;&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;GCC String Truncation Checkers&lt;/h2&gt; &lt;p&gt;GCC has two checkers that detect string truncation bugs: &lt;code&gt;&lt;tt&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-8.1.0/gcc/Warning-Options.html#index-Wformat_002dtruncation"&gt;-Wformat-truncation&lt;/a&gt;&lt;/tt&gt;&lt;/code&gt; (first introduced in GCC 7) and &lt;code&gt;&lt;tt&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-8.1.0/gcc/Warning-Options.html#index-Wstringop_002dtruncation"&gt;-Wstringop-truncation&lt;/a&gt;&lt;/tt&gt;&lt;/code&gt; (new in GCC 8). &lt;code&gt;&lt;tt&gt;-Wformat-truncation&lt;/tt&gt;&lt;/code&gt; detects truncation by the &lt;code&gt;&lt;tt&gt;snprintf&lt;/tt&gt;&lt;/code&gt; family of standard input/output functions, and &lt;code&gt;&lt;tt&gt;-Wstringop-truncation&lt;/tt&gt;&lt;/code&gt; detects the same problem by the &lt;code&gt;&lt;tt&gt;strncat&lt;/tt&gt;&lt;/code&gt; and &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt; functions. The warnings are closely related to but distinct from &lt;code&gt;&lt;tt&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-8.1.0/gcc/Warning-Options.html#index-Wformat_002doverflow"&gt;-Wformat-overflow&lt;/a&gt;&lt;/tt&gt;&lt;/code&gt; and &lt;code&gt;&lt;tt&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-8.1.0/gcc/Warning-Options.html#index-Wstringop_002doverflow"&gt;-Wstringop-overflow&lt;/a&gt;&lt;/tt&gt;&lt;/code&gt;, which detect buffer overflow by the corresponding unbounded standard I/O functions and by string-modifying functions declared in &lt;code&gt;&lt;tt&gt;&amp;#60;string.h&amp;#62;&lt;/tt&gt;&lt;/code&gt;, respectively. All of these warnings, although conceptually simple, rely heavily on advanced data and control flow analyses performed by a number of optimization passes within GCC to maximize efficacy.&lt;/p&gt; &lt;h2&gt;Forming Truncated Strings with &lt;tt&gt;snprintf&lt;/tt&gt;&lt;/h2&gt; &lt;p&gt;Forming a string that is shorter than expected is the most common kind of truncation. It typically results from calls to functions such as &lt;code&gt;&lt;tt&gt;snprintf&lt;/tt&gt;&lt;/code&gt; and &lt;code&gt;&lt;tt&gt;strncat&lt;/tt&gt;&lt;/code&gt;. The result is a valid string in the sense that it is properly terminated by the NUL character, but its length is less than expected because it is missing one or more trailing characters. For instance, if a string represents a name, truncating it may result in it matching a different name, as in the following example:&lt;/p&gt; &lt;pre&gt; char dirname[256]; char filename[256]; FILE* open_file (void) { char pathname[256]; snprintf (pathname, sizeof pathname, "%s/%i/%s", dirname, getpid (), filename); return fopen (pathname, "w"); }&lt;/pre&gt; &lt;p&gt;If the concatenation of the five components of the pathname does not fit in 256 bytes, the result will not refer to the intended file. A &lt;code&gt;&lt;tt&gt;dirname&lt;/tt&gt;&lt;/code&gt; that is close to 256 characters long means the PID may end up truncated, and the file created would not just have the wrong name but it would also be in the wrong directory. If the file contains sensitive data, another process, possibly one controlled by a hacker, may be able to read or manipulate it in illicit ways. To help detect this problem, GCC diagnoses the &lt;code&gt;&lt;tt&gt;snprintf&lt;/tt&gt;&lt;/code&gt; call above with a message similar to this:&lt;/p&gt; &lt;pre&gt;warning: '%i' directive output may be truncated writing between 1 and 11 bytes into a region of size between 0 and 255 [-Wformat-truncation=] snprintf (pathname, sizeof pathname, "%s/%i/%s", dirname, getpid (), filename); ^~ note: 'snprintf' output between 4 and 524 bytes into a destination of size 256 snprintf (pathname, sizeof pathname, "%s/%i/%s", dirname, getpid (), filename); ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/pre&gt; &lt;p&gt;The warning uses string lengths and ranges of integer arguments to determine when truncation is possible. When the length of a string argument cannot be determined and the argument is an array of known size, the warning uses the size of the array as the worst-case estimate of the string length instead. In the instance above, the text of the warning indicates that if &lt;code&gt;&lt;tt&gt;dirname&lt;/tt&gt;&lt;/code&gt; were as few as 244 characters long, appending the slash and a very large PID value close to &lt;code&gt;&lt;tt&gt;INT_MAX&lt;/tt&gt;&lt;/code&gt; would result in truncating the PID, not to mention the final slash and &lt;code&gt;&lt;tt&gt;filename&lt;/tt&gt;&lt;/code&gt;. The phrasing &amp;#8220;&lt;i&gt;output may be truncated&lt;/i&gt;&amp;#8221; indicates that truncation is possible but not inevitable. If the truncation were certain the phrase &amp;#8220;&lt;i&gt;output truncated&lt;/i&gt;&amp;#8221; would be used instead. The final note then gives the minimum and maximum number of characters that GCC has determined the function will write into the destination.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;&lt;tt&gt;-Wformat-truncation&lt;/tt&gt;&lt;/code&gt; warning is not new in GCC 8, but thanks to some enhancements it is able to detect more instances of the problem than GCC 7. It detects so many that users tend to be surprised by the volume of warnings for their code. Some users have complained that they find the warning too noisy. The most common complaint is that it points out code where the result of truncation either is not used in ways that would cause it to misbehave (such as to print a message on the terminal or into a log file where it doesn&amp;#8217;t matter that a part of the string is cut off) or that the truncation is handled later (such as by the &lt;code&gt;&lt;tt&gt;fopen&lt;/tt&gt;&lt;/code&gt; function failing to open a file with a truncated name).&lt;/p&gt; &lt;p&gt;Such views neglect to consider that incomplete or truncated messages make program output difficult for users or (in the case of log files) for operators to interpret. In more severe cases (such as relying on &lt;code&gt;&lt;tt&gt;fopen&lt;/tt&gt;&lt;/code&gt; to fail to open a file), they tend to underestimate the downstream security risks.&lt;/p&gt; &lt;p&gt;However, despite best efforts, &lt;code&gt;&lt;tt&gt;-Wformat-truncation&lt;/tt&gt;&lt;/code&gt; is not free of real false positives (instances when, by design, the warning should not be issued but is as a result of GCC bugs or limitations). Although they are proving harder to avoid than we would like, none is due to inherent flaws in GCC architecture or design but rather due to limitations in various optimization passes. As frustrating (and time consuming) as false positives can be both for users and for GCC developers, in this case they help highlight possible code generation improvement opportunities that might otherwise go unnoticed. GCC developers are tracking these false positives along with the optimization opportunities and working toward solutions.&lt;/p&gt; &lt;p&gt;Irrespective of whether a given instance of the &lt;code&gt;&lt;tt&gt;-Wformat-truncation&lt;/tt&gt;&lt;/code&gt; warning indicates a possible bug in a program or it is a false positive, for best results it is best to avoid truncation. Since the purpose of &lt;code&gt;&lt;tt&gt;snprintf&lt;/tt&gt;&lt;/code&gt; is to prevent buffer overflow, we recommend developers assume that every non-trivial call to &lt;code&gt;&lt;tt&gt;snprintf&lt;/tt&gt;&lt;/code&gt; can result in truncation (otherwise, using the function would be unnecessary) and handle it appropriately. When GCC detects that truncation cannot happen, it will optimize the handling away, eliminating any overhead that might otherwise result.&lt;/p&gt; &lt;h3&gt;Using &lt;tt&gt;snprintf&lt;/tt&gt; Safely&lt;/h3&gt; &lt;p&gt;One way to avoid truncation is to use &lt;code&gt;&lt;tt&gt;snprintf&lt;/tt&gt;&lt;/code&gt; to determine the size of the destination buffer before storing output in it, and allocate it dynamically so it is just large enough. This is done by calling the function twice: once with a null destination pointer and then again with a pointer to the allocated buffer. The buffer can be allocated either by &lt;code&gt;&lt;tt&gt;malloc&lt;/tt&gt;&lt;/code&gt; or, when its size is known to be small enough, as a variable-length array (use &lt;code&gt;&lt;tt&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-8.1.0/gcc/Warning-Options.html#index-Wvla"&gt;-Wvla-larger-than&lt;/a&gt;&lt;/tt&gt;&lt;/code&gt; to detect excessively large VLAs), for example, like this:&lt;/p&gt; &lt;pre&gt; FILE* open_file (void) { errno = 0; int n = snprintf (0, 0, "%s/%i/%s", dirname, getpid (), filename); if (n &amp;#60; 0) { perror ("snprintf failed"); abort (); } char *pathname = (char *) malloc (n + 1); if (!pathname) { perror ("malloc failed"); abort (); } FILE *fp = fopen (pathname, "w"); free (pathname); return fp; }&lt;/pre&gt; &lt;h3&gt;Avoiding Truncation with &lt;tt&gt;sprintf&lt;/tt&gt; and &lt;tt&gt;open_memstream&lt;/tt&gt;&lt;/h3&gt; &lt;p&gt;Another solution is to use the POSIX &lt;code&gt;&lt;tt&gt;open_memstream&lt;/tt&gt;&lt;/code&gt; function to create a &lt;code&gt;&lt;tt&gt;FILE&lt;/tt&gt;&lt;/code&gt; object that, when used with other I/O functions, manages a dynamically allocated buffer that grows as necessary to fit all output. With this approach, it is of course necessary to handle running out of memory, for example:&lt;/p&gt; &lt;pre&gt; FILE* open_file (void) { char *pathname; size_t pathsize; FILE *pathfp = open_memstream (&amp;#38;pathname, &amp;#38;pathsize); if (!pathfp) { perror ("open_memstream failed"); abort (); } fprintf (pathfp, "%s/%i/%s", dirname, getpid (), filename); if (fclose (pathfp)) { // Likely out of memory. perror ("fclose failed"); abort (); } FILE *fp = fopen (pathname, "w"); free (pathname); return fp;&lt;/pre&gt; &lt;h3&gt;Avoiding Truncation with &lt;tt&gt;asprintf&lt;/tt&gt;&lt;/h3&gt; &lt;p&gt;Finally, the BSD and GNU function &lt;code&gt;&lt;tt&gt;asprintf&lt;/tt&gt;&lt;/code&gt; can safely be used to both dynamically allocate the buffer and format output in a single call. As with the &lt;code&gt;&lt;tt&gt;open_memstream&lt;/tt&gt;&lt;/code&gt; approach, the trade-off with &lt;code&gt;&lt;tt&gt;asnprintf&lt;/tt&gt;&lt;/code&gt; is that callers must be prepared to handle the function&amp;#8217;s failure to allocate memory (that is, to detect and handle &lt;code&gt;&lt;tt&gt;ENOMEM&lt;/tt&gt;&lt;/code&gt;).&lt;/p&gt; &lt;h3&gt;Handling Truncation When It Occurs&lt;/h3&gt; &lt;p&gt;When avoiding truncation is not feasible, it needs to be handled. To handle &lt;code&gt;&lt;tt&gt;snprintf&lt;/tt&gt;&lt;/code&gt; truncation, the value returned from the function must be used to take some action. GCC looks to see whether the value is used in a meaningful way and avoids issuing the warning when it is. Note that it is not sufficient to assign the returned value to an otherwise unused variable. The simplest, though not necessarily the most appropriate, way to handle truncation is to abort, for example:&lt;/p&gt; &lt;pre&gt; FILE* open_file (void) { char pathname[256]; int n = snprintf (pathname, sizeof pathname, "%s/%i/%s", dirname, getpid (), filename); if (n &amp;#60; 0) { perror ("snprintf failed"); abort (); } if ((size_t)n &amp;#62; sizeof pathname) { perror ("pathname too long"); abort (); } return fopen (pathname, "w"); }&lt;/pre&gt; &lt;h2&gt;Forming Truncated Strings with &lt;tt&gt;strncat&lt;/tt&gt;&lt;/h2&gt; &lt;p&gt;String truncation can also occur as a result of calling &lt;code&gt;&lt;tt&gt;strncat&lt;/tt&gt;&lt;/code&gt;. The origins of &lt;code&gt;&lt;tt&gt;strncat&lt;/tt&gt;&lt;/code&gt; (and &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt;) can be traced to &lt;a href="https://en.wikipedia.org/wiki/Version_7_Unix"&gt;Version 7 UNIX&lt;/a&gt;, which was released in 1979 and in which functions were introduced to manipulate arrays of binary data not necessarily terminated by the NUL character, such as directory entries or encryption keys. Unlike the other functions discussed in this article, &lt;code&gt;&lt;tt&gt;strncat&lt;/tt&gt;&lt;/code&gt; is impossible to use safely even in the originally intended cases. To be used safely, the function would need to take as arguments not just the size of the remaining space in the destination but also the maximum number of characters to copy from a non-string. However, by providing only one size argument, it is impossible to avoid both buffer overflow and string truncation. Since preventing buffer overflow tends to be viewed as more important than preventing string truncation, GCC assumes the size argument refers to the remaining space in the destination buffer and expects safe calls to match the following pattern (see also the US-CERT article on &lt;a title="strncpy() and strncat()" href="https://www.us-cert.gov/bsi/articles/knowledge/coding-practices/strncpy-and-strncat"&gt;&lt;code&gt;strncpy()&lt;/code&gt; and &lt;code&gt;strncat()&lt;/code&gt;&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt; strncat (dest, src, strlen (dest) - 1);&lt;/pre&gt; &lt;p&gt;Calls that have this form are not diagnosed. Other calls, such as those where the size is derived in some way from the size or length of the source string, are diagnosed by &lt;code&gt;&lt;tt&gt;-Wstringop-overflow&lt;/tt&gt;&lt;/code&gt;. That includes unsafe calls like&lt;/p&gt; &lt;pre&gt; strncat (dest, src, strlen (src));&lt;/pre&gt; &lt;p&gt;and&lt;/p&gt; &lt;pre&gt; strncat (dest, src, sizeof src);&lt;/pre&gt; &lt;h2&gt;Forming Non-NUL-Terminated Sequences&lt;/h2&gt; &lt;p&gt;An entirely different form of string truncation is one that can result from calls to &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt;. Unlike functions such as &lt;code&gt;&lt;tt&gt;snprintf&lt;/tt&gt;&lt;/code&gt; and &lt;code&gt;&lt;tt&gt;strncat&lt;/tt&gt;&lt;/code&gt; that always append a terminating NUL, when the source string passed to &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt; is longer than the size specified by the third argument, the function truncates the copy without appending a NUL to the end. The result is not a string in the C or C++ sense (which is defined as a NUL-terminated sequence of bytes in both languages) and, thus, it is not suitable as an argument to functions that expect one. It is a common error to call a string-handling function such as &lt;code&gt;&lt;tt&gt;strlen&lt;/tt&gt;&lt;/code&gt; with an argument that is not a NUL-terminated string, for example:&lt;/p&gt; &lt;pre&gt; FILE* open_file (const char *dirname, const char *filename) { char pathname[256]; strncpy (pathname, dirname, sizeof pathname); strncat (pathname, "/", sizeof pathname); strncat (pathname, filename, sizeof pathname); return fopen (pathname, "w"); }&lt;/pre&gt; &lt;p style="text-align: left;"&gt;If &lt;code&gt;&lt;tt&gt;dirname&lt;/tt&gt;&lt;/code&gt; is longer than 255 characters, the call to &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt; will copy the first 256 characters from it to &lt;code&gt;&lt;tt&gt;pathname&lt;/tt&gt;&lt;/code&gt; without adding a terminating NUL. The subsequent calls to &lt;tt&gt;strncat&lt;/tt&gt; will then try to write the path separator and the contents of &lt;code&gt;&lt;tt&gt;filename&lt;/tt&gt;&lt;/code&gt; somewhere past the end of the &lt;code&gt;&lt;tt&gt;pathname&lt;/tt&gt;&lt;/code&gt; buffer. Where exactly that occurs depends on the contents of memory beyond the end of the buffer (the location of the first NUL byte). GCC helps detect these errors by diagnosing the code like this:&lt;/p&gt; &lt;pre&gt;warning: 'strncpy' specified bound 256 equals destination size [-Wstringop-truncation] strncpy (pathname, dirname, sizeof pathname); ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ warning: 'strncat' specified bound 256 equals destination size [-Wstringop-overflow=] strncat (pathname, filename, sizeof pathname); ^~~~~~~&lt;/pre&gt; &lt;p&gt;The first warning is the one relevant to the case we are discussing. (Note that this warning may be suppressed due to GCC bug &lt;a title="missing -Wstringop-truncation on strncpy due to system header macro " href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82944"&gt;82944&lt;/a&gt;.) The second warning is caused by improperly bounding the number of characters copied by &lt;code&gt;&lt;tt&gt;strncat&lt;/tt&gt;&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Using &lt;tt&gt;strncpy&lt;/tt&gt; Safely&lt;/h3&gt; &lt;p&gt;In general, it is not possible to avoid string truncation by &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt; except by sizing the destination to be at least a byte larger than the length of the source string. With that approach, however, using &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt; becomes unnecessary and the function can be avoided in favor of other APIs such as &lt;code&gt;&lt;tt&gt;strcpy&lt;/tt&gt;&lt;/code&gt; or (less preferably) &lt;code&gt;&lt;tt&gt;memcpy&lt;/tt&gt;&lt;/code&gt;. Much has been written about the problems with &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt; and we recommend to avoid it whenever possible. It is, however, worth keeping in mind that unlike other standard string-handling functions, &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt; always writes exactly as many characters as specified by the third argument; if the source string is shorter, the function fills the remaining bytes with NULs.&lt;/p&gt; &lt;h3&gt;Mitigating &lt;tt&gt;strncpy&lt;/tt&gt; Truncation&lt;/h3&gt; &lt;p&gt;Since it is not possible to avoid truncation by &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt;, when using other functions is not feasible, it is necessary to make sure the result of &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt; is properly NUL-terminated and the NUL must be inserted explicitly, after &lt;code&gt;&lt;tt&gt;strncpy&lt;/tt&gt;&lt;/code&gt; has returned:&lt;/p&gt; &lt;pre&gt; char pathname[256]; strncpy (pathname, dirname, sizeof pathname); pathname[sizeof pathname - 1] = '\0';&lt;/pre&gt; &lt;p&gt;GCC tries to detect these uses and avoid issuing the warning when it can determine that the NUL is inserted before the array is used by a string-handling function. However, the simple approach outlined above suffers from the same problem as ignoring &lt;code&gt;&lt;tt&gt;snprintf&lt;/tt&gt;&lt;/code&gt; truncation and so, to be safe, the truncation should be detected and handled as discussed above. GCC 8 doesn&amp;#8217;t detect the missing handling in this case but future versions might.&lt;/p&gt; &lt;p&gt;Avoiding the possible buffer overflow in the subsequent &lt;code&gt;&lt;tt&gt;strncat&lt;/tt&gt;&lt;/code&gt; calls in the example is left as an exercise for the reader.&lt;/p&gt; &lt;p&gt;&lt;code&gt;&lt;tt&gt;-Wstringop-truncation&lt;/tt&gt;&lt;/code&gt; is arguably less prone to false negatives than the other warnings discussed in this article, but perhaps even more so than &lt;code&gt;&lt;tt&gt;-Wformat-truncation&lt;/tt&gt;&lt;/code&gt;, it can be prone to false positives. That is because the originally intended and safe uses of the function are not always distinguishable from the unsafe ones. It was a necessary judgment call to decide whether to issue diagnostics in those cases. GCC developers decided to err on the side of caution and issue the warning on the basis that false positives are easy to suppress, especially by experienced programmers, in the correct and safe uses, while the false negatives would let mistakes by less experienced or less careful programmers go unnoticed. To help differentiate between the two sets of use cases and allow the false positives to be avoided, GCC 8 has introduced a new attribute to decorate arrays and pointers with that need not be NUL-terminated. The name of the attribute is &lt;code&gt;&lt;tt&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-8.1.0/gcc/Common-Variable-Attributes.html#index-nonstring-variable-attribute"&gt;nonstring&lt;/a&gt;&lt;/tt&gt;&lt;/code&gt;, and it is used by GCC to suppress select &lt;code&gt;&lt;tt&gt;-Wformat-truncation&lt;/tt&gt;&lt;/code&gt; instances where the missing NUL is intended. It is important to note that since character arrays that are not NUL-terminated are not valid arguments to functions that expect strings (such as &lt;code&gt;&lt;tt&gt;strlen&lt;/tt&gt;&lt;/code&gt; or &lt;code&gt;&lt;tt&gt;strcpy&lt;/tt&gt;&lt;/code&gt;), using &lt;code&gt;&lt;tt&gt;nonstring&lt;/tt&gt;&lt;/code&gt; arrays with such functions is diagnosed.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F24%2Fdetecting-string-truncation-with-gcc-8%2F&amp;#38;linkname=Detecting%20String%20Truncation%20with%20GCC%208" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F24%2Fdetecting-string-truncation-with-gcc-8%2F&amp;#38;linkname=Detecting%20String%20Truncation%20with%20GCC%208" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F24%2Fdetecting-string-truncation-with-gcc-8%2F&amp;#38;linkname=Detecting%20String%20Truncation%20with%20GCC%208" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F24%2Fdetecting-string-truncation-with-gcc-8%2F&amp;#38;linkname=Detecting%20String%20Truncation%20with%20GCC%208" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F24%2Fdetecting-string-truncation-with-gcc-8%2F&amp;#38;linkname=Detecting%20String%20Truncation%20with%20GCC%208" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F24%2Fdetecting-string-truncation-with-gcc-8%2F&amp;#38;linkname=Detecting%20String%20Truncation%20with%20GCC%208" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F24%2Fdetecting-string-truncation-with-gcc-8%2F&amp;#38;linkname=Detecting%20String%20Truncation%20with%20GCC%208" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F24%2Fdetecting-string-truncation-with-gcc-8%2F&amp;#38;linkname=Detecting%20String%20Truncation%20with%20GCC%208" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F24%2Fdetecting-string-truncation-with-gcc-8%2F&amp;#38;title=Detecting%20String%20Truncation%20with%20GCC%208" data-a2a-url="https://developers.redhat.com/blog/2018/05/24/detecting-string-truncation-with-gcc-8/" data-a2a-title="Detecting String Truncation with GCC 8"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/24/detecting-string-truncation-with-gcc-8/"&gt;Detecting String Truncation with GCC 8&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/jecJ3uw8Cdk" height="1" width="1" alt=""/&gt;</content><summary>Continuing in the effort to detect common programming errors, the just-released GCC 8 contains a number of new warnings as well as enhancements to existing checkers to help find non-obvious bugs in C and C++ code. This article focuses on those that deal with inadvertent string truncation and discusses some of the approaches for avoiding the underlying problems. If you haven’t read it, you might al...</summary><dc:creator>unknown</dc:creator><dc:date>2018-05-24T10:55:15Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/24/detecting-string-truncation-with-gcc-8/</feedburner:origLink></entry><entry><title>Scotland JBug - AppDev in the Cloud Workshop</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/AhySuccyGXI/scotland-jbug-appdev-in-the-cloud-workshop.html" /><category term="AppDev" scheme="searchisko:content:tags" /><category term="BRMS" scheme="searchisko:content:tags" /><category term="cloud" scheme="searchisko:content:tags" /><category term="Cloud Suite" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="event" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="jbug" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="workshops" scheme="searchisko:content:tags" /><category term="xpaas" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-scotland_jbug_appdev_in_the_cloud_workshop</id><updated>2018-05-24T05:00:12Z</updated><published>2018-05-24T05:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;a href="https://www.soprasteria.co.uk/en/newsroom/event/2018/06/06/default-calendar/sopra-steria-sponsors-jbug-scotland-appdev-in-the-cloud-workshop" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="256" data-original-width="256" height="200" src="https://3.bp.blogspot.com/-hcg0RzC1ifY/VMZDoi_rtZI/AAAAAAAAd2w/X0ZkKnXLKNc-o8eGvDWXQpMIRwy_4Av7ACPcBGAYYCw/s200/jbug-scotland.jpeg" width="200" /&gt;&lt;/a&gt;&lt;br /&gt;I'm &lt;a href="https://www.soprasteria.co.uk/en/newsroom/event/2018/06/06/default-calendar/sopra-steria-sponsors-jbug-scotland-appdev-in-the-cloud-workshop" target="_blank"&gt;heading back to my friends in Scotland&lt;/a&gt; to speak at the JBug Scotland next month.&lt;br /&gt;&lt;br /&gt;It's a fun group of people that really seem to enjoy working with open source and JBoss software stacks.&lt;br /&gt;&lt;br /&gt;This time around I'm bringing a full hands-on workshop showcasing application development in the cloud using containers, JBoss middleware, services, business logic and APIs.&lt;br /&gt;&lt;br /&gt;The event is on June 6th, 2018 from 16:15 to 19:30 or so... &lt;a href="https://www.soprasteria.co.uk/en/newsroom/event/2018/06/06/default-calendar/sopra-steria-sponsors-jbug-scotland-appdev-in-the-cloud-workshop" target="_blank"&gt;be sure to register online&lt;/a&gt; so they can order the right amount of food.&lt;br /&gt;&lt;br /&gt;The abstract for the workshop and talk:&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;&lt;h3 style="text-align: left;"&gt;&lt;a href="https://appdevcloudworkshop.github.io/#/" target="_blank"&gt;AppDev in the Cloud Workshop&lt;/a&gt;&lt;/h3&gt;&lt;i&gt;Ready to get hands-on with AppDev in the Cloud with container based services?&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt; &lt;i&gt;This workshop will let you experience the wonders of Red Hat's open technologies for cloud-based container application development, letting you integrate multiple services in to a polyglot cloud solution. In this workshop you're a developer working for Destinaisa, a travel agency that needs to set up its online bookings applications backend services. You'll be given the OpenShift Container Platform, then installing JBoss BRMS to work in the Destinasia discount rules.&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt; &lt;i&gt;Once they are completed, you leverage Ansible playbooks to see infrastructure automation in action. Each playbook will deploy a new container based service to support flight, hotel, car and discount rule queries from your application. In total you will be running 6 container based applications or services in a private PaaS before testing this solution with a REST client, sending a booking and verifying the discounts provided by the rules you implemented.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;Bring your laptop and see you in Scotland next month!&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=SyPwH6Wye10:HsvxMWNiQPY:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=SyPwH6Wye10:HsvxMWNiQPY:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=SyPwH6Wye10:HsvxMWNiQPY:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=SyPwH6Wye10:HsvxMWNiQPY:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=SyPwH6Wye10:HsvxMWNiQPY:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=SyPwH6Wye10:HsvxMWNiQPY:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=SyPwH6Wye10:HsvxMWNiQPY:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=SyPwH6Wye10:HsvxMWNiQPY:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=SyPwH6Wye10:HsvxMWNiQPY:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=SyPwH6Wye10:HsvxMWNiQPY:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=SyPwH6Wye10:HsvxMWNiQPY:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/SyPwH6Wye10" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/AhySuccyGXI" height="1" width="1" alt=""/&gt;</content><summary>I'm heading back to my friends in Scotland to speak at the JBug Scotland next month. It's a fun group of people that really seem to enjoy working with open source and JBoss software stacks. This time around I'm bringing a full hands-on workshop showcasing application development in the cloud using containers, JBoss middleware, services, business logic and APIs. The event is on June 6th, 2018 from ...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2018-05-24T05:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/SyPwH6Wye10/scotland-jbug-appdev-in-the-cloud-workshop.html</feedburner:origLink></entry><entry><title>Querying Debezium Change Data Events With KSQL</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/AHTI6hdGNzI/" /><category term="example" scheme="searchisko:content:tags" /><category term="feed_group_name_debezium" scheme="searchisko:content:tags" /><category term="feed_name_debezium" scheme="searchisko:content:tags" /><category term="ksql" scheme="searchisko:content:tags" /><category term="mysql" scheme="searchisko:content:tags" /><author><name>Jiri Pechanec</name></author><id>searchisko:content:id:jbossorg_blog-querying_debezium_change_data_events_with_ksql</id><updated>2018-05-24T09:27:11Z</updated><published>2018-05-24T00:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Last year we have seen the inception of a new open-source project in the &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; universe, &lt;a href="https://github.com/confluentinc/ksql"&gt;KSQL&lt;/a&gt;, which is a streaming SQL engine build on top of &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams&lt;/a&gt;. In this post, we are going to try out KSQL querying with data change events generated by Debezium from a MySQL database.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As a source of data we will use the database and setup from our &lt;a href="http://debezium.io/docs/tutorial"&gt;tutorial&lt;/a&gt;. The result of this exercise should be similar to the recent &lt;a href="http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/"&gt;post&lt;/a&gt; about aggregation of events into &lt;a href="https://martinfowler.com/bliki/DDD_Aggregate.html"&gt;domain driven aggregates&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="entity_diagram"&gt;&lt;a class="anchor" href="#entity_diagram"&gt;&lt;/a&gt;Entity diagram&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;First let’s look at the entities in the database and the relations between them.&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock centered-image"&gt; &lt;div class="content"&gt; &lt;img src="http://debezium.io/images/tutorial-erd.svg" alt="Entity diagram" /&gt; &lt;/div&gt; &lt;div class="title"&gt;Figure 1: Entity diagram of the example entities&lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt; &lt;br /&gt;&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The picture above shows the full ER diagram for the inventory database in the example MySQL instance. We are going to focus on two entities:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;customers&lt;/code&gt; - the list of customers in the system&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;orders&lt;/code&gt; - the list of orders in the system&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;There is a &lt;code&gt;1:n&lt;/code&gt; relation between &lt;code&gt;customers&lt;/code&gt; and &lt;code&gt;orders&lt;/code&gt;, modelled by the &lt;code&gt;purchaser&lt;/code&gt; column in the &lt;code&gt;orders&lt;/code&gt; table, which is a foreign key to the &lt;code&gt;customers&lt;/code&gt; table.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="configuration"&gt;&lt;a class="anchor" href="#configuration"&gt;&lt;/a&gt;Configuration&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We are going to use a &lt;a href="https://github.com/debezium/debezium-examples/blob/master/tutorial/docker-compose-mysql.yaml"&gt;Docker Compose file&lt;/a&gt; for the deployment of the environment. The deployment consists of the following Docker images:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://hub.docker.com/r/debezium/zookeeper/"&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://hub.docker.com/r/debezium/kafka/"&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Kafka Connect including the Debezium connectors &lt;a href="https://hub.docker.com/r/debezium/connect/"&gt;image&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A pre-populated MySQL database as used in our &lt;a href="http://debezium.io/docs/tutorial"&gt;tutorial&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We also need the KSQL client. To make things simple we are going to use a pre-built &lt;a href="https://hub.docker.com/r/confluentinc/ksql-cli/"&gt;Docker image&lt;/a&gt; but you can download and directly use the client from the KSQL &lt;a href="https://github.com/confluentinc/ksql/releases"&gt;download&lt;/a&gt; page.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="example"&gt;&lt;a class="anchor" href="#example"&gt;&lt;/a&gt;Example&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;First we need to start the Debezium and Kafka infrastructure. To do so, clone the &lt;a href="https://github.com/debezium/debezium-examples/"&gt;debezium-examples&lt;/a&gt; GitHub repository and start the required components using the provided Compose file:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;export DEBEZIUM_VERSION=0.7 git clone https://github.com/debezium/debezium-examples.git cd debezium-examples/tutorial/ docker-compose -f docker-compose-mysql.yaml up&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Next we must register an instance of the Debezium MySQL connector to listen to changes in the database:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" http://localhost:8083/connectors/ -d @- &amp;lt;&amp;lt;-EOF { "name": "inventory-connector", "config": { "connector.class": "io.debezium.connector.mysql.MySqlConnector", "tasks.max": "1", "database.hostname": "mysql", "database.port": "3306", "database.user": "debezium", "database.password": "dbz", "database.server.id": "184055", "database.server.name": "dbserver", "database.whitelist": "inventory", "database.history.kafka.bootstrap.servers": "kafka:9092", "database.history.kafka.topic": "schema-changes.inventory", "transforms": "unwrap", "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope", "key.converter": "org.apache.kafka.connect.json.JsonConverter", "key.converter.schemas.enable": "false", "value.converter": "org.apache.kafka.connect.json.JsonConverter", "value.converter.schemas.enable": "false" } } EOF&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Now we should have all components up and running and initial data change events are already streamed into Kafka topics. There are multiple properties that are especially important for our use case:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The &lt;a href="http://debezium.io/docs/configuration/event-flattening/"&gt;UnwrapFromEnvelope SMT&lt;/a&gt; is used. This allows us to directly map fields from the &lt;code&gt;after&lt;/code&gt; part of change records into KSQL statements. Without it, we would need to use &lt;code&gt;EXTRACTJSONFIELD&lt;/code&gt; for each field to be extracted from the &lt;code&gt;after&lt;/code&gt; part of messages.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Schemas are disabled for the JSON converter. The reason is the same as above. With schemas enabled, for JSON the record is encapsulated in a JSON structure that contains the fields &lt;code&gt;schema&lt;/code&gt; (with schema information) and &lt;code&gt;payload&lt;/code&gt; (with the actual data itself). We would again need to use &lt;code&gt;EXTRACTJSONFIELD&lt;/code&gt; to get to the relevant fields. There is no such issue with Avro converter so this option does not need to be set when Avro is used.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Next we are going to start the KSQL command shell. We will run a local engine in the CLI. Also please note &lt;code&gt;--net&lt;/code&gt; parameter. This guarantees that KSQL container runs in the same network as Debezium containers and allows proper DNS resolution.&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;docker run -it --net tutorial_default confluentinc/ksql-cli ksql-cli local --bootstrap-server kafka:9092&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;First we will list all Kafka topics that exist in the broker:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;ksql&amp;gt; LIST TOPICS; Kafka Topic | Registered | Partitions | Partition Replicas ------------------------------------------------------------------------------------ connect-status | false | 5 | 1 dbserver | false | 1 | 1 dbserver.inventory.addresses | false | 1 | 1 dbserver.inventory.customers | false | 1 | 1 dbserver.inventory.orders | false | 1 | 1 dbserver.inventory.products | false | 1 | 1 dbserver.inventory.products_on_hand | false | 1 | 1 ksql__commands | true | 1 | 1 my_connect_configs | false | 1 | 1 my_connect_offsets | false | 25 | 1 schema-changes.inventory | false | 1 | 1&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The topics we are interested in are &lt;code&gt;dbserver.inventory.orders&lt;/code&gt; and &lt;code&gt;dbserver.inventory.customers&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;KSQL processing by default starts with &lt;code&gt;latest&lt;/code&gt; offsets. We want to process the events already in the topics so we switch processing from &lt;code&gt;earliest&lt;/code&gt; offsets.&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;ksql&amp;gt; SET 'auto.offset.reset' = 'earliest'; Successfully changed local property 'auto.offset.reset' from 'null' to 'earliest'&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;First we need to create streams from the topics containing the Debezium data change events. A &lt;em&gt;stream&lt;/em&gt; in KSQL and Kafka Streams terminology is an unbounded incoming data set with no state.&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;ksql&amp;gt; CREATE STREAM orders_from_debezium (order_number integer, order_date string, purchaser integer, quantity integer, product_id integer) WITH (KAFKA_TOPIC='dbserver.inventory.orders',VALUE_FORMAT='json'); Message ---------------- Stream created ksql&amp;gt; ksql&amp;gt; CREATE STREAM customers_from_debezium (id integer, first_name string, last_name string, email string) WITH (KAFKA_TOPIC='dbserver.inventory.customers',VALUE_FORMAT='json'); Message ---------------- Stream created&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="partitioning"&gt;&lt;a class="anchor" href="#partitioning"&gt;&lt;/a&gt;Partitioning&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Our deployment uses only one partition per topic. In a production system there will likely be multiple partitions per topic and we need to ensure that all events belonging to our aggregated object end up in the same partition. The natural partioning in our case is per customer id. We are going to repartition the &lt;code&gt;orders_from_debezium&lt;/code&gt; stream according to the &lt;code&gt;purchaser&lt;/code&gt; field that contains the customer id. The repartitioned data are written into a new topic &lt;code&gt;ORDERS_REPART&lt;/code&gt;:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;ksql&amp;gt; CREATE STREAM orders WITH (KAFKA_TOPIC='ORDERS_REPART',VALUE_FORMAT='json',PARTITIONS=1) as SELECT * FROM orders_from_debezium PARTITION BY PURCHASER; Message ---------------------------- Stream created and running ksql&amp;gt; LIST TOPICS; Kafka Topic | Registered | Partitions | Partition Replicas ------------------------------------------------------------------------------------ ... ORDERS_REPART | true | 1 | 1 ...&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We are going to execute the same operation for customers too. It is necessary for two reasons:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The current key is a struct that contains a field named &lt;code&gt;id&lt;/code&gt; with the customer id. This is different from the repartitioned order topic which contains only the &lt;code&gt;id&lt;/code&gt; value as the key, so the partitions would not match.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;When we will create a JOIN later, there is a limitation that requires the key to have the same value as a key field in the table. The table field contains a plain value but the key contains a struct so they would not match. See &lt;a href="https://github.com/confluentinc/ksql/issues/749"&gt;this KSQL issue&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;ksql&amp;gt; CREATE STREAM customers_stream WITH (KAFKA_TOPIC='CUSTOMERS_REPART',VALUE_FORMAT='json',PARTITIONS=1) as SELECT * FROM customers_from_debezium PARTITION BY ID; Message ---------------------------- Stream created and running ksql&amp;gt; LIST TOPICS; Kafka Topic | Registered | Partitions | Partition Replicas ------------------------------------------------------------------------------------ ... CUSTOMERS_REPART | true | 1 | 1 ...&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;To verify that records have a new key and are thus repartioned we can issue few statements to compare the results:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;ksql&amp;gt; SELECT * FROM orders_from_debezium LIMIT 1; 1524034842810 | {"order_number":10001} | 10001 | 16816 | 1001 | 1 | 102 LIMIT reached for the partition. Query terminated ksql&amp;gt; SELECT * FROM orders LIMIT 1; 1524034842810 | 1001 | 10001 | 16816 | 1001 | 1 | 102 LIMIT reached for the partition. Query terminated&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The second column contains &lt;code&gt;ROWKEY&lt;/code&gt; which is the key of the message.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect3"&gt; &lt;h4 id="customer_order_join"&gt;&lt;a class="anchor" href="#customer_order_join"&gt;&lt;/a&gt;Customer/order join&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;So far we were only declaring streams as an unbounded stateless data set. In our use case the &lt;code&gt;order&lt;/code&gt; is really an event that comes and goes. But &lt;code&gt;customer&lt;/code&gt; is an entity that can be updated and generally is a part of a state fo the system. Such quality is represented in KSQL or Kafka Streams as table. We are going to create a table of customers from the topic containing repartitioned customers.&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;ksql&amp;gt; CREATE TABLE customers (id integer, first_name string, last_name string, email string) WITH (KAFKA_TOPIC='CUSTOMERS_REPART',VALUE_FORMAT='json',KEY='id'); Message --------------- Table created&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Now we have everything in place to make a join between customer and its orders and create a query that will monitor incoming orders and list them with associated customer fields.&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;ksql&amp;gt; SELECT order_number,quantity,customers.first_name,customers.last_name FROM orders left join customers on orders.purchaser=customers.id; 10001 | 1 | Sally | Thomas 10002 | 2 | George | Bailey 10003 | 2 | George | Bailey 10004 | 1 | Edward | Walker&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Let’s apply a few changes to the database, which will result in corresponding CDC events being emitted by Debezium:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;docker-compose -f docker-compose-mysql.yaml exec mysql bash -c 'mysql -u $MYSQL_USER -p$MYSQL_PASSWORD inventory' mysql&amp;gt; INSERT INTO orders VALUES(default,NOW(), 1003,5,101); Query OK, 1 row affected, 1 warning (0.02 sec) mysql&amp;gt; UPDATE customers SET first_name='Annie' WHERE id=1004; Query OK, 1 row affected (0.02 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql&amp;gt; UPDATE orders SET quantity=20 WHERE order_number=10004; Query OK, 1 row affected (0.02 sec) Rows matched: 1 Changed: 1 Warnings: 0&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You may notice that only changes in the &lt;code&gt;orders&lt;/code&gt; table have triggered changes in the joined stream. This is a product of the stream/table join. We would need a stream/stream join to trigger changes if any of input streams is modified.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;So the final result of the select after the database is modified is&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;10001 | 1 | Sally | Thomas 10002 | 2 | George | Bailey 10003 | 2 | George | Bailey 10004 | 1 | Edward | Walker 10005 | 5 | Edward | Walker 10004 | 20 | Edward | Walker&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="summary"&gt;&lt;a class="anchor" href="#summary"&gt;&lt;/a&gt;Summary&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We have successfully started a KSQL instance. We have mapped KSQL streams to Debezium topics filled by Debezium and made a join between them. We have also discussed the problem of repartioning in streaming applications.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If you’d like to try out this example with Avro encoding and schema registry then you can use our &lt;a href="https://github.com/debezium/debezium-examples/blob/master/tutorial/docker-compose-mysql-avro.yaml"&gt;Avro example&lt;/a&gt;. Also for further details and more advanced usages just refer to the KSQL &lt;a href="https://github.com/confluentinc/ksql/blob/master/docs/syntax-reference.md"&gt;syntax reference&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In case you need help, have feature requests or would like to share your experiences with this example, please let us know in the comments below.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="about_debezium"&gt;&lt;a class="anchor" href="#about_debezium"&gt;&lt;/a&gt;About Debezium&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href="http://kafka.apache.org/"&gt;Kafka&lt;/a&gt; and provides &lt;a href="http://kafka.apache.org/documentation.html#connect"&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href="http://debezium.io/license"&gt;open source&lt;/a&gt; under the &lt;a href="http://www.apache.org/licenses/LICENSE-2.0.html"&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="get_involved"&gt;&lt;a class="anchor" href="#get_involved"&gt;&lt;/a&gt;Get involved&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href="https://twitter.com/debezium"&gt;@debezium&lt;/a&gt;, &lt;a href="https://gitter.im/debezium/user"&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href="https://groups.google.com/forum/#!forum/debezium"&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href="https://github.com/debezium/"&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href="https://issues.jboss.org/projects/DBZ/issues/"&gt;log an issue&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/AHTI6hdGNzI" height="1" width="1" alt=""/&gt;</content><summary>Last year we have seen the inception of a new open-source project in the Apache Kafka universe, KSQL, which is a streaming SQL engine build on top of Kafka Streams. In this post, we are going to try out KSQL querying with data change events generated by Debezium from a MySQL database. As a source of data we will use the database and setup from our tutorial. The result of this exercise should be si...</summary><dc:creator>Jiri Pechanec</dc:creator><dc:date>2018-05-24T00:00:00Z</dc:date><feedburner:origLink>http://debezium.io/blog/2018/05/24/querying-debezium-change-data-eEvents-with-ksql/</feedburner:origLink></entry><entry><title>Teiid 10.2.2 Released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/n1OvAxefZRM/teiid-1022-released.html" /><category term="feed_group_name_teiid" scheme="searchisko:content:tags" /><category term="feed_name_teiid" scheme="searchisko:content:tags" /><author><name>Steven Hawkins</name></author><id>searchisko:content:id:jbossorg_blog-teiid_10_2_2_released</id><updated>2018-05-23T18:52:45Z</updated><published>2018-05-23T18:52:00Z</published><content type="html">&lt;a href="http://teiid.io/teiid_four_ways/teiid_wildfly/downloads/"&gt;Teiid 10.2.2&lt;/a&gt; addresses 14 issues: &lt;br /&gt;&lt;ul&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5336"&gt;TEIID-5336&lt;/a&gt;] - Improve TEIID-5253 &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-4784"&gt;TEIID-4784&lt;/a&gt;] - Provide functionality to perform RENAME table in DDL scripts &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5324"&gt;TEIID-5324&lt;/a&gt;] - MongoDB: SecurityType "None" is not working &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5328"&gt;TEIID-5328&lt;/a&gt;] - regression of org.teiid.padSpace does not affect to the "IN" operator behavior &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5329"&gt;TEIID-5329&lt;/a&gt;] - Problem with salesforce url &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5330"&gt;TEIID-5330&lt;/a&gt;] - FIRST_VALUE/LAST_VALUE/LEAD/LAG functions always try to return integer &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5331"&gt;TEIID-5331&lt;/a&gt;] - LEAD/LAG ignores ORDER BY in the OVER clause &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5333"&gt;TEIID-5333&lt;/a&gt;] - Complex foreign keys set the referenced key regardless of order &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5334"&gt;TEIID-5334&lt;/a&gt;] - Improve pg/ODBC mapping of char type &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5335"&gt;TEIID-5335&lt;/a&gt;] - "No value was available" in ROW_NUMBER while inserting in foreign temporary table &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5339"&gt;TEIID-5339&lt;/a&gt;] - Vertica join query fails due to unexpected ordering of intermediate results &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5342"&gt;TEIID-5342&lt;/a&gt;] - If excel FIRST_DATA_ROW_NUMBER is past all rows, the last row is still used &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5345"&gt;TEIID-5345&lt;/a&gt;] - ClassCastException if multi-column dependent join is pushed to literals &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5347"&gt;TEIID-5347&lt;/a&gt;] - low level MetadataFactory properties not honored by DDL import &lt;/li&gt;&lt;/ul&gt; You can expect the next fix release in 4-5 weeks. Thank you, Steve&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/n1OvAxefZRM" height="1" width="1" alt=""/&gt;</content><summary>Teiid 10.2.2 addresses 14 issues: [TEIID-5336] - Improve TEIID-5253 [TEIID-4784] - Provide functionality to perform RENAME table in DDL scripts [TEIID-5324] - MongoDB: SecurityType "None" is not working [TEIID-5328] - regression of org.teiid.padSpace does not affect to the "IN" operator behavior [TEIID-5329] - Problem with salesforce url [TEIID-5330] - FIRST_VALUE/LAST_VALUE/LEAD/LAG functions alw...</summary><dc:creator>Steven Hawkins</dc:creator><dc:date>2018-05-23T18:52:00Z</dc:date><feedburner:origLink>http://teiid.blogspot.com/2018/05/teiid-1022-released.html</feedburner:origLink></entry><entry><title>Customizing an OpenShift Ansible Playbook Bundle</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/B7RQA9cGh7w/" /><category term="ansible" scheme="searchisko:content:tags" /><category term="apb" scheme="searchisko:content:tags" /><category term="asb" scheme="searchisko:content:tags" /><category term="broker" scheme="searchisko:content:tags" /><category term="bundle" scheme="searchisko:content:tags" /><category term="container" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="Deployment" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="OpenShift Enterprise by Red Hat" scheme="searchisko:content:tags" /><category term="paas" scheme="searchisko:content:tags" /><category term="platform" scheme="searchisko:content:tags" /><category term="playbook" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="service" scheme="searchisko:content:tags" /><author><name>Alessandro Arrichiello</name></author><id>searchisko:content:id:jbossorg_blog-customizing_an_openshift_ansible_playbook_bundle</id><updated>2018-05-23T13:30:11Z</updated><published>2018-05-23T13:30:11Z</published><content type="html">&lt;p&gt;Today I want to talk about Ansible Service Broker and Ansible Playbook Bundle. These components are relatively new in the Red Hat OpenShift ecosystem, but they are now fully supported features available in the Service Catalog component of OpenShift 3.9.&lt;/p&gt; &lt;p&gt;Before getting deep into the technology, I want to give you some basic information (quoted below from the product documentation) about all the components and their features:&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;Ansible Service Broker is an implementation of the &lt;a href="https://github.com/openservicebrokerapi/servicebroker"&gt;Open Service Broker API&lt;/a&gt; that manages applications defined in &lt;a href="https://github.com/ansibleplaybookbundle/ansible-playbook-bundle"&gt;Ansible Playbook Bundles&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Ansible Playbook Bundles (APB) are a method of defining applications via a collection of Ansible Playbooks built into a container with an Ansible runtime with the playbooks corresponding to a type of request specified in the &lt;a href="https://github.com/openservicebrokerapi/servicebroker/blob/master/spec.md#api-overview"&gt;Open Service Broker API specification&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Playbooks are Ansible’s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;span id="more-495887"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;So the ASB (Ansible Service Broker) is the man-in-the-middle between the APB (Ansible Playbook Bundle) and a third-party user that would like to consume the service offered through the &lt;a href="http://docs.ansible.com/ansible/latest/user_guide/playbooks.html"&gt;Ansible Playbook&lt;/a&gt; on OpenShift.&lt;/p&gt; &lt;p&gt;Linking up these two components, OpenShift Service Catalog is able to offer—through OpenShift Web Portal and its API—access to these pieces of deployment/configuration to OpenShift users. This enables an entire world of possibilities from an OpenShift perspective:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Easily define, distribute, and provision microservice(s), such as &lt;a href="https://github.com/ansibleplaybookbundle/rocketchat-apb"&gt;RocketChat&lt;/a&gt; and &lt;a href="https://github.com/ansibleplaybookbundle/postgresql-apb"&gt;PostgreSQL&lt;/a&gt;, via Ansible Playbooks packaged in Ansible Playbook Bundles.&lt;/li&gt; &lt;li&gt;Easily bind microservice(s) provisioned through Ansible Playbook Bundles, for example, as shown in this video: &lt;a href="https://www.youtube.com/watch?v=xmd52NhEjCk" rel="nofollow"&gt;Using the Service Catalog to Bind a PostgreSQL APB to a Python Web App&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Getting deeper into the technology, we&amp;#8217;ll see how to use the following steps to create a MariaDB APB that will set up MariaDB on an external remote host:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Get started with the technology by creating your first APB.&lt;/li&gt; &lt;li&gt;Customize your first APB and let it configure an external remote host (through SSH).&lt;/li&gt; &lt;li&gt;Build and push your first APB.&lt;/li&gt; &lt;li&gt;Understand the ASB and APB operations under the hood.&lt;/li&gt; &lt;li&gt;Troubleshoot an APB.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Are you ready? Let&amp;#8217;s get started!&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Please note:&lt;/strong&gt; You need to have a fully configured and functional OpenShift 3.9 cluster before continuing. Minishift and the CDK, at the moment, do not offer Service Catalog and Ansible Service Broker enabled. Please check the project documentation.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;Getting Started with an APB&lt;/h2&gt; &lt;p&gt;Starting from OpenShift 3.9, you&amp;#8217;ll not need any additional configuration or deployment to get Ansible Service Broker and the Service Catalog working. They will be set up by the OpenShift installer at first installation/update.&lt;/p&gt; &lt;p&gt;So, the first thing you need to do is to let the Ansible Service Broker search in the OpenShift default registry for container images. To achieve this, edit the &lt;code&gt;configmap&lt;/code&gt; used by the Ansible Service Broker and edit the whitelist:&lt;/p&gt; &lt;pre&gt;$ oc edit configmap broker-config -n openshift-ansible-service-broker&lt;/pre&gt; &lt;p&gt;In the &lt;code&gt;configmap&lt;/code&gt;, add a whitelist rule for the OpenShift registry similar to the one already set up for the Docker Hub registry:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;&amp;#8211; type: local_openshift&lt;br /&gt; name: localregistry&lt;br /&gt; namespaces:&lt;br /&gt; &amp;#8211; openshift&lt;br /&gt; white_list:&lt;br /&gt; &lt;strong&gt;&amp;#8211; &amp;#8220;.*-apb$&amp;#8221;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;With this rule in place, the Ansible Service Broker will search in the local OpenShift registry for container images  ending with the string &lt;code&gt;-apb&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You&amp;#8217;re now ready to initialize your first APB. First, you need the &lt;code&gt;apb&lt;/code&gt; binary. On a Red Hat Enterprise Linux (with OpenShift repos enabled), you just need to run this command:&lt;/p&gt; &lt;pre&gt;$ yum install -y apb&lt;/pre&gt; &lt;p&gt;Then you can initialize your first APB by running this command:&lt;/p&gt; &lt;pre&gt;$ apb init mariadb-deployment-apb&lt;/pre&gt; &lt;p&gt;The command will set up an initial directory tree (shown below), which is ready to be customized depending on your needs:&lt;/p&gt; &lt;pre&gt;$ ls -la mariadb-deployment-apb/ total 12 drwxrwxr-x. 4 ocpadmin ocpadmin 85 May 18 14:08 . drwx------. 9 ocpadmin ocpadmin 255 May 18 14:33 .. -rw-rw-r--. 1 ocpadmin ocpadmin 1082 May 18 13:57 apb.yml -rw-rw-r--. 1 ocpadmin ocpadmin 1731 May 18 14:34 Dockerfile -rw-rw-r--. 1 ocpadmin ocpadmin 769 May 16 12:16 Makefile drwxrwxr-x. 2 ocpadmin ocpadmin 50 May 18 14:33 playbooks drwxrwxr-x. 6 ocpadmin ocpadmin 145 May 16 12:30 roles&lt;/pre&gt; &lt;p&gt;As you can see, the command creates a description file called &lt;code&gt;apb.yml&lt;/code&gt; for metadata and parameters that need to be requested from users of the playbook bundle.&lt;b&gt; &lt;/b&gt;The metadata will be used for displaying the item in the ServiceCatalog, while the parameters will be used to prompt users of the bundle to supply necessary configuration details. We&amp;#8217;ll take a look and customize it in the next section.&lt;/p&gt; &lt;p&gt;Then it creates a Dockerfile for building up the final container and a Makefile, of course, that defines the method for building and pushing the container up to the OpenShift internal registry.&lt;/p&gt; &lt;p&gt;Finally, you&amp;#8217;ll find the two key directories containing—guess what?— Ansible &amp;#8220;playbooks&amp;#8221; and &amp;#8220;roles.&amp;#8221; These directories contain pre-built playbooks for provisioning and de-provisioning and a skeleton for a custom role you may want to build.&lt;/p&gt; &lt;p&gt;But let&amp;#8217;s take a look to the playbook it made:&lt;/p&gt; &lt;pre&gt;$ cat playbooks/provision.yml - name: mariadb-test-apb playbook to provision the application   hosts: localhost   gather_facts: false   connection: local   roles:   - role: ansible.kubernetes-modules     install_python_requirements: no   - role: ansibleplaybookbundle.asb-modules   - role: provision-mariadb-test-apb&lt;/pre&gt; &lt;p&gt;As you can see, the playbook is really simple. It runs against localhost (&lt;code&gt;connection: local&lt;/code&gt;), and it will execute two pre-defined roles: &lt;code&gt;ansible.kubernetes-modules&lt;/code&gt; and &lt;code&gt;ansibleplaybookbundle.asb-modules&lt;/code&gt;. These two roles will set up the basic actions for letting your container communicate with the current OpenShift platform and its underlying Kubernetes layer.&lt;/p&gt; &lt;p&gt;Finally, the playbook will execute a custom role, &lt;code&gt;provision-mariadb-test-apb&lt;/code&gt;. This role is basically empty; you should fill it with your code!&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;Customizing Your First APB for Connecting to a Remote Host&lt;/h2&gt; &lt;p&gt;As mentioned in the introduction, you will not use the standard behavior for your APB. Instead, you&amp;#8217;ll make it connect to an external host for installing and configuring MariaDB.&lt;/p&gt; &lt;p&gt;First, you need to edit the &lt;code&gt;apb.yml&lt;/code&gt; file to add some metadata and variables that you&amp;#8217;ll use later in the playbooks:&lt;/p&gt; &lt;pre&gt;$ cat apb.yml version: 1.0 name: mariadb-deployment-apb description: This is a sample application generated by apb init bindable: False async: optional metadata:   displayName: MariaDB on vm (APB) plans:   - name: default     description: This default plan deploys mariadb-deployment-apb     free: True     metadata: {}     parameters:       - name: dbname         title: Database name to create on just created mariadb         type: string         default: myappdb         required: true       - name: rootpassword         title: Database root password to set         type: string         default: P4ssw0rd!         required: true         display_type: password       - name: target_host         title: Target Host for provisioning         type: string         default: 172.16.0.7         required: true       - name: remoteuser         title: SSH Remote User         type: string         default: user         required: true       - name: sshprivkey         title: SSH Private key for connecting to the remote machine         type: string         required: true         display_type: textarea&lt;/pre&gt; &lt;p&gt;As you can see, you set up five parameters that the user will be asked to provide through the OpenShift interface:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The database name&lt;/li&gt; &lt;li&gt;The root password for the database&lt;/li&gt; &lt;li&gt;The target host to connect to&lt;/li&gt; &lt;li&gt;The remote user to use during SSH connection&lt;/li&gt; &lt;li&gt;The SSH private key to use during SSH connection&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;As you may suppose, you&amp;#8217;ll not write from scratch a role for installing and configuring MariaDB on your remote system. There are tons of roles available on the Ansible Galaxy network!&lt;/p&gt; &lt;p&gt;For this example, I chose these two (you&amp;#8217;ll need a role for configuring the firewall, too):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://galaxy.ansible.com/bertvv/mariadb"&gt;MariaDB Ansible role&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://galaxy.ansible.com/geerlingguy/firewall"&gt;Firewall Ansible role&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Download them and place them under &lt;code&gt;roles&lt;/code&gt; directory.&lt;/p&gt; &lt;p&gt;After that, you need to edit the &lt;code&gt;provision.yml&lt;/code&gt; playbook. For connecting to an external host, you need to add the host to the inventory, dynamically.&lt;/p&gt; &lt;pre&gt;$ cat playbooks/provision.yml - name: mariadb-deployment-apb playbook to provision the application hosts: localhost gather_facts: false connection: local roles: - role: ansible.kubernetes-modules install_python_requirements: no - role: ansibleplaybookbundle.asb-modules - role: provision-mariadb-deployment-apb playbook_debug: false tasks: - name: Adding the remote host to the inventory add_host: name: "{{ target_host }}" groups: target_group changed_when: false - name: Adding ssh private key shell: "mkdir -p /opt/apb/.ssh &amp;#38;&amp;#38; chmod 700 /opt/apb/.ssh &amp;#38;&amp;#38; echo -e \"{{ sshprivkey }}\" &amp;#62; /opt/apb/.ssh/id_rsa &amp;#38;&amp;#38; chmod 600 /opt/apb/.ssh/id_rsa" - name: Provision mariadb hosts: target_group remote_user: "{{ remoteuser }}" become: true vars: firewall_allowed_tcp_ports: - "22" - "3306" mariadb_bind_address: "0.0.0.0" mariadb_root_password: "{{ rootpassword }}" mariadb_databases: - name: "{{ dbname }}" roles: - role: ansible-role-firewall - role: ansible-role-mariadb&lt;/pre&gt; &lt;p&gt;Inspecting the playbook, you&amp;#8217;ll see that first you add the host (from the variable) to the inventory, and then you set up the SSH private key for connecting to the remote host. To accomplish this, I use a single shell command instead of taking the command apart using all the available Ansible modules.&lt;/p&gt; &lt;p&gt;Then in the second playbook, you connect to the remote host to configure the firewall and MariaDB.&lt;/p&gt; &lt;p&gt;So, you&amp;#8217;ll also need to edit the &lt;code&gt;deprovision.yml&lt;/code&gt; playbook:&lt;/p&gt; &lt;pre&gt;$ cat playbooks/deprovision.yml - name: mariadb-deployment-apb playbook to deprovision the application hosts: localhost gather_facts: false connection: local roles: - role: ansible.kubernetes-modules install_python_requirements: no - role: ansibleplaybookbundle.asb-modules - role: deprovision-mariadb-deployment-apb playbook_debug: false tasks: - name: Adding the remote host to the inventory add_host: name: "{{ target_host }}" groups: target_group changed_when: false - name: Adding ssh private key shell: "mkdir -p /opt/apb/.ssh &amp;#38;&amp;#38; chmod 700 /opt/apb/.ssh &amp;#38;&amp;#38; echo -e \"{{ sshprivkey }}\" &amp;#62; /opt/apb/.ssh/id_rsa &amp;#38;&amp;#38; chmod 600 /opt/apb/.ssh/id_rsa" - name: Remove mariadb packages remote_user: "{{ remoteuser }}" become: yes hosts: target_group tasks: - name: Remove the package from the host package: name: mariadb state: absent&lt;/pre&gt; &lt;p&gt;The deprovisioning is just to remove the &lt;code&gt;mariadb&lt;/code&gt; package and nothing else.&lt;/p&gt; &lt;p&gt;Finally, for connecting smoothly to your remote host without SSH prompting us to add  the host to the known list, you can make a little addition to the Dockerfile to disable host_key_checking:&lt;/p&gt; &lt;pre&gt;$ cat ../mariadb-test-apb/Dockerfile FROM ansibleplaybookbundle/apb-base LABEL "com.redhat.apb.spec"=\ COPY playbooks /opt/apb/actions COPY roles /opt/ansible/roles &lt;strong&gt;RUN echo "host_key_checking = False" &amp;#62;&amp;#62; /opt/apb/ansible.cfg&lt;/strong&gt; RUN chmod -R g=u /opt/{ansible,apb} USER apb&lt;/pre&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;Building and Pushing Your First APB&lt;/h2&gt; &lt;p&gt;First, you need to prepare the APB for the push to the registry:&lt;/p&gt; &lt;pre&gt;$ apb prepare Finished writing dockerfile.&lt;/pre&gt; &lt;p&gt;This command adds a signature to the Dockerfile so you can double-checking the build later.&lt;/p&gt; &lt;p&gt;After that, you can build the APB. Remember you need to be &lt;code&gt;root&lt;/code&gt; (or have proper rights for accessing the Docker daemon):&lt;/p&gt; &lt;pre&gt;$ sudo apb build Finished writing dockerfile. Building APB using tag: [mariadb-deployment-apb] Successfully built APB image: mariadb-deployment-apb&lt;/pre&gt; &lt;p&gt;You can double-check the build by checking the list of docker images:&lt;/p&gt; &lt;pre&gt;$ sudo docker images|grep mariadb mariadb-deployment-apb latest b4d6a95a79b7 2 days ago 604 MB&lt;/pre&gt; &lt;p&gt;And finally, you can push the APB into the registry. But before proceeding, you should be logged in to OpenShift as an admin user with a valid token. The user &lt;code&gt;system:admin&lt;/code&gt; doesn&amp;#8217;t have a token by default, so create an additional user and give it the &lt;code&gt;cluster-admin&lt;/code&gt;&amp;#8221; role.&lt;/p&gt; &lt;pre&gt;$ sudo oc whoami ocpadmin $ sudo apb push Didn't find OpenShift Ansible Broker route in namespace: ansible-service-broker. Trying openshift-ansible-service-broker version: 1.0 name: mariadb-deployment-apb description: This is a sample application generated by apb init bindable: False async: optional metadata: displayName: MariaDB on vm (APB) plans: - name: default description: This default plan deploys mariadb-deployment-apb free: True metadata: {} parameters: - name: dbname title: Database name to create on just created mariadb type: string default: myappdb required: true - name: rootpassword title: Database root password to set type: string default: R3dh4t1! required: true display_type: password - name: target_host title: Target Host for provisioning type: string default: 172.16.0.7 required: true - name: remoteuser title: SSH Remote User type: string default: user required: true - name: sshprivkey title: SSH Private key for connecting to the remote machine type: string required: true display_type: textarea Found registry IP at: 172.30.3.246:5000 Finished writing dockerfile. Building APB using tag: [172.30.3.246:5000/openshift/mariadb-deployment-apb] Successfully built APB image: 172.30.3.246:5000/openshift/mariadb-deployment-apb Found image: docker-registry.default.svc:5000/openshift/mariadb-deployment-apb Warning: Tagged image registry prefix doesn't match. Deleting anyway. Given: 172.30.3.246:5000; Found: docker-registry.default.svc:5000 Successfully deleted sha256:0bd78762bbf717333f1e017e3578bcd55a70877810fc7f859d04455e80df0a94 Pushing the image, this could take a minute... Successfully pushed image: 172.30.3.246:5000/openshift/mariadb-deployment-apb Contacting the ansible-service-broker at: https://asb-1338-openshift-ansible-service-broker.140.11.34.16.nip.io/ansible-service-broker/v2/bootstrap Successfully bootstrapped Ansible Service Broker Successfully relisted the Service Catalog&lt;/pre&gt; &lt;p&gt;You did it! You successfully loaded an APB into the OpenShift registry and Ansible Service Broker.&lt;/p&gt; &lt;p&gt;The next time you log in to OpenShift, you should find the APB in the Service Catalog:&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-full wp-image-496167 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console-1024x446.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console-300x131.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console-768x334.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console-1024x446.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/p&gt; &lt;p&gt;And moving forward, in the Configuration section, you&amp;#8217;ll see the required variables you configured earlier:&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-full wp-image-496177 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console1.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console1.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console1.png 903w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console1-300x224.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console1-768x573.png 768w" sizes="(max-width: 903px) 100vw, 903px" /&gt;&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;Understanding the ASB and APB Operations Under the Hood&lt;/h2&gt; &lt;p&gt;Requesting the element from the Service Catalog by clicking &lt;strong&gt;Create&lt;/strong&gt; in the previous screen, will start a very special action inside the running OpenShift cluster:&lt;/p&gt; &lt;pre&gt;$ oc get serviceinstance -n test NAME AGE localregistry-mariadb-deployment-apb-bd4cb 27s $ oc get pods --all-namespaces|grep apb localregistry-mariadb-deployment-apb-prov-6pntf apb-f34a346d-3b25-46a5-95c2-54d480ae6701 1/1 Running 0 28s&lt;/pre&gt; &lt;p&gt;As you can see, an element of type &lt;code&gt;ServiceInstance&lt;/code&gt; was created and linked to this, a new pod was scheduled: our Ansible playbook is just running in this container.&lt;/p&gt; &lt;p&gt;Looking through the logs, you can monitor the running activities:&lt;/p&gt; &lt;pre&gt;$ oc logs -n localregistry-mariadb-deployment-apb-prov-6pntf apb-f34a346d-3b25-46a5-95c2-54d480ae6701 -f PLAY [mariadb-deployment-apb playbook to provision the application] ************ TASK [ansible.kubernetes-modules : Install latest openshift client] ************ skipping: [localhost] TASK [ansibleplaybookbundle.asb-modules : debug] ******************************* skipping: [localhost] TASK [Adding the remote host to the inventory] ********************************* ok: [localhost] TASK [Adding ssh private key] ************************************************** [WARNING]: Consider using the file module with state=directory rather than running mkdir. If you need to use command because file is insufficient you can add warn=False to this command task or set command_warnings=False in ansible.cfg to get rid of this message. changed: [localhost] PLAY [Provision mariadb] ******************************************************* TASK [Gathering Facts] ********************************************************* ok: [10.1.0.11] TASK [ansible-role-firewall : Ensure iptables is present.] ********************* ok: [10.1.0.11] TASK [ansible-role-firewall : Flush iptables the first time playbook runs.] **** ok: [10.1.0.11] TASK [ansible-role-firewall : Copy firewall script into place.] **************** ok: [10.1.0.11] TASK [ansible-role-firewall : Copy firewall init script into place.] *********** skipping: [10.1.0.11] TASK [ansible-role-firewall : Copy firewall systemd unit file into place (for systemd systems).] *** ok: [10.1.0.11] TASK [ansible-role-firewall : Configure the firewall service.] ***************** ok: [10.1.0.11] ...&lt;/pre&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;Troubleshooting an APB&lt;/h2&gt; &lt;p&gt;What happens if your tests go wrong and the Ansible pod fails and produces an error?&lt;/p&gt; &lt;p&gt;Of course you can look through all the APB pods (as shown before) and run the usual &lt;code&gt;oc debug PODNAME&lt;/code&gt; command for creating a brand-new pod for troubleshooting.&lt;/p&gt; &lt;p&gt;If you experience some issue deleting a failed &lt;code&gt;ServiceInstance&lt;/code&gt;, you can edit the element to remove the Kubernetes finalizer, as shown below:&lt;/p&gt; &lt;pre&gt;$ oc get serviceinstance -n test -o yaml apiVersion: v1 items: - apiVersion: servicecatalog.k8s.io/v1beta1 kind: ServiceInstance metadata: creationTimestamp: 2018-05-20T21:14:23Z finalizers: - kubernetes-incubator/service-catalog generateName: localregistry-mariadb-deployment-apb- generation: 1 name: localregistry-mariadb-deployment-apb-bd4cb namespace: test ...&lt;/pre&gt; &lt;p&gt;Sometimes, some nodes can get a different version of your APB in the Docker cache, so you might experience different behaviors if you did multiple builds and pushes. You can manually log in to the various OpenShift nodes and then clean up the outdated Docker images (you may want use Ansible from bastion host for doing that).&lt;/p&gt; &lt;p&gt;That&amp;#8217;s all!&lt;/p&gt; &lt;p&gt;Feel free to ask if you have any questions!&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;About Alessandro&lt;/h2&gt; &lt;p class="selectionShareable"&gt;&lt;img class="wp-image-496187 alignleft" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture.jpg" alt="" width="180" height="181" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture.jpg 957w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-150x150.jpg 150w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-300x300.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-768x770.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-32x32.jpg 32w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-50x50.jpg 50w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-64x64.jpg 64w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-96x96.jpg 96w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-128x128.jpg 128w" sizes="(max-width: 180px) 100vw, 180px" /&gt;Alessandro Arrichiello is a Solution Architect for Red Hat Inc. He has a passion for GNU/Linux systems, which began at age 14 and continues today. He works with tools for automating enterprise IT: configuration management and continuous integration through virtual platforms. He’s now working on a distributed cloud environment involving PaaS (OpenShift), IaaS (OpenStack) and processes management (CloudForms), container building, instance creation, HA services management, and workflow builds.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;title=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" data-a2a-url="https://developers.redhat.com/blog/2018/05/23/customizing-an-openshift-ansible-playbook-bundle/" data-a2a-title="Customizing an OpenShift Ansible Playbook Bundle"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/23/customizing-an-openshift-ansible-playbook-bundle/"&gt;Customizing an OpenShift Ansible Playbook Bundle&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/B7RQA9cGh7w" height="1" width="1" alt=""/&gt;</content><summary>Today I want to talk about Ansible Service Broker and Ansible Playbook Bundle. These components are relatively new in the Red Hat OpenShift ecosystem, but they are now fully supported features available in the Service Catalog component of OpenShift 3.9. Before getting deep into the technology, I want to give you some basic information (quoted below from the product documentation) about all the com...</summary><dc:creator>Alessandro Arrichiello</dc:creator><dc:date>2018-05-23T13:30:11Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/23/customizing-an-openshift-ansible-playbook-bundle/</feedburner:origLink></entry><entry><title>Behind the scenes of Red Hat Summit 2018 scheduling</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/0tVMjmyUNqw/BehindTheScenesOfRedHatSummitScheduling.html" /><category term="feed_group_name_optaplanner" scheme="searchisko:content:tags" /><category term="feed_name_optaplanner" scheme="searchisko:content:tags" /><category term="useCase" scheme="searchisko:content:tags" /><author><name>unknown</name></author><id>searchisko:content:id:jbossorg_blog-behind_the_scenes_of_red_hat_summit_2018_scheduling</id><updated>2018-05-23T16:33:00Z</updated><published>2018-05-23T00:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Earlier this month, Red Hat organized it’s annual Summit conference in San Francisco for more than 7000 attendees. As &lt;a href="https://youtu.be/r8e4bT0-zhU?t=1m47s"&gt;Jim Whitehurst explained in his opening keynote&lt;/a&gt;, OptaPlanner optimized attendee experience by scheduling all of the 325 non-keynote sessions. Let’s take a look behind the scenes.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_the_challenge_in_theory"&gt;The challenge (in theory)&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A few weeks after the CFP closed and the program group decided which talks to accepts, Arrie Brown (master coordinator of Summit, the business expert) and me (the technical expert) started the automatic scheduling. So for everyone wondering why your brilliant talk wasn’t accepted: &lt;em&gt;it’s not OptaPlanner’s fault!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Instead, it had to assign a timeslot and room to each talk. So for everyone wondering why your brilliant talk didn’t get a better time: &lt;em&gt;it is OptaPlanner’s fault!&lt;/em&gt; My own talk was on the last timeslot of the first day, from 16:30 until 17:15. Traitor. 12 years of dedication to its code and this how it repays me?&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Anyway, given this kind of input:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="summitConferenceScheduling_0.png" alt="summitConferenceScheduling 0"&gt; &lt;/img&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We generated this kind of output:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="summitConferenceScheduling_1.png" alt="summitConferenceScheduling 1"&gt; &lt;/img&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Seems easy, right?&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Of course, there’s a long list of constraints. For starters, there are 5 different talk types (Breakout, Panel, Birds of a Feather, Lab, Workshops, Mini Session) with different durations and different room requirements. A 2 hour Lab doesn’t fit in a 20 minute timeslot. But a Mini Session does. And that’s just the tip of the constraints iceberg:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="summitConferenceScheduling_2.png" alt="summitConferenceScheduling 2"&gt; &lt;/img&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Arrie created a Google Docs spreadsheet, uploaded the data from the CFP application and ran it through the &lt;a href="https://www.optaplanner.org/learn/useCases/conferenceScheduling.html"&gt;OptaPlanner Conference Scheduling example&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_the_challenge_in_practice"&gt;The challenge (in practice)&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="sect2"&gt; &lt;h3 id="_the_pigeonhole_principle"&gt;The pigeonhole principle&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The first road bump we encountered was that OptaPlanner couldn’t find a feasible schedule. It scheduled dozens of talks in the same room at the same time. That’s a problem. &lt;a href="https://www.optaplanner.org/blog/2018/02/19/SchedulingVoxxedDaysZurich2018.html"&gt;Voxxed Zurich 2018&lt;/a&gt;, didn’t run into this problem, but it only had 1 talk type. It took us a while to figure out the cause, due to talk type complexity, the sheer size of the conference and especially the poor visualization at the time. I even got side-tracked on trying to fix the feasibility, instead of prioritizing the visualization first, for more insight into the result quality.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Once I improved the visualization in the output spreadsheet, the real problem surfaced immediately: &lt;a href="https://en.wikipedia.org/wiki/Pigeonhole_principle"&gt;not enough pigeon holes&lt;/a&gt;. There were 325 talks and only 300 slots to put them in.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We added the 2 missing rooms in the input data and got our first feasible solution. But we didn’t just want a workable schedule: we wanted a great schedule.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_garbage_in_garbage_out"&gt;Garbage in, garbage out&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The CFP application is pretty lenient. During the first import of that data into the OptaPlanner example, Arrie already fixed a bunch of data issues (such as duplicate speaker rows), to get the spreadsheet to import successfully.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;But not all data issues can be detected automatically. For example, it doesn’t automatically know which talks are popular and need a bigger room. Two rooms were already tagged as &lt;code&gt;Large&lt;/code&gt;. Based on experience of previous editions, she identified 7 talks that required such a &lt;code&gt;Large&lt;/code&gt; room. She configured their &lt;code&gt;Required room tags&lt;/code&gt; accordingly.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This kind of experienced, human input is vital to get a good schedule.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_tweaking_the_constraint_weights"&gt;Tweaking the constraint weights&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Attendees should be able to see all talks that they want to see. Ideally, we’d use the mobile app data to know which talks each attendee bookmarked, but that data is only available shortly before the conference, long after the schedule is published. That’s a catch 22. Instead, we avoid scheduling talks at the same time if they cover the same theme track, sector or content, so every attendee can attend all talks on a particular topic.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;These 3 tag types (theme track, sector, content) are not equally important. Some conflicts (2 talks at the same time) are worse than others. There are only a few theme tracks and each talk has multiple of those, so theme track conflicts are common. Therefore, a conflict of 2 talks with the &lt;em&gt;Containers&lt;/em&gt; theme track is less important than a conflict of 2 talks with the &lt;em&gt;Kubernetes&lt;/em&gt; content tag.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The spreadsheet defines the weight of each constraint, allowing to simulate the impact of different weights on the generated schedule. After some tweaking, avoiding content tag conflicts ended up 50 times as important as avoiding theme track conflicts.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Some content tags were very common. For example, 46 talks included the content tag &lt;code&gt;OpenShift&lt;/code&gt;. Others only appeared 2 or 3 times. I’d now argue that a conflict with 2 (out of 46) &lt;code&gt;OpenShift&lt;/code&gt; talks is less important than a conflict with the &lt;em&gt;only&lt;/em&gt; 2 talks of another content tag. So for next year, we might want to normalize the impact of every content tag, based on the number of talks with that tag.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_pinning_as_a_workaround"&gt;Pinning as a workaround&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;There was one talk that gave us grief, at least for a moment: 2 breakout sessions that were part 1 and part 2 of the same talk. Part 2 needed to start when part 1 ended, in the same room. To proceed quickly, we just &lt;em&gt;pinned&lt;/em&gt; those 2 session manually to a room and timeslots, before solving it. It worked. OptaPlanner scheduled all other talks while respecting those 2 pre-set assignments.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Of course, if we do see more of these 2-part session cases, I 'll add a new constraint to deal with it properly.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We generated a good, fair schedule with the &lt;a href="https://www.optaplanner.org/learn/useCases/conferenceScheduling.html"&gt;OptaPlanner Conference Scheduling example&lt;/a&gt;. Similar to Google Search, we only had to define what we want, not how to look for it.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Once we got the input data in a good state, we just had to press the &lt;em&gt;Solve&lt;/em&gt; button and give it some time. Next year, we’ll be able to reuse this code. If you’re organizing a conference, take a look at the video below to try it out yourself.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_related_video"&gt;Related video&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt; &lt;iframe width="853" height="480" src="https://www.youtube.com/embed/R0JizNdxEjU" frameborder="0" allowfullscreen="" /&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/0tVMjmyUNqw" height="1" width="1" alt=""/&gt;</content><summary>Earlier this month, Red Hat organized it’s annual Summit conference in San Francisco for more than 7000 attendees. As Jim Whitehurst explained in his opening keynote, OptaPlanner optimized attendee experience by scheduling all of the 325 non-keynote sessions. Let’s take a look behind the scenes. The challenge (in theory) A few weeks after the CFP closed and the program group decided which talks to...</summary><dc:creator>unknown</dc:creator><dc:date>2018-05-23T00:00:00Z</dc:date><feedburner:origLink>https://www.optaplanner.org/blog/2018/05/23/BehindTheScenesOfRedHatSummitScheduling.html</feedburner:origLink></entry><entry><title>Using Ansible Galaxy Roles in Ansible Playbook Bundles</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KhvDAScZbGM/" /><category term="ansible" scheme="searchisko:content:tags" /><category term="apb" scheme="searchisko:content:tags" /><category term="Cloud Automation" scheme="searchisko:content:tags" /><category term="Cloud Services" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Open Service Broker" scheme="searchisko:content:tags" /><category term="OpenShift Enterprise by Red Hat" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><author><name>Siamak Sadeghianfar</name></author><id>searchisko:content:id:jbossorg_blog-using_ansible_galaxy_roles_in_ansible_playbook_bundles</id><updated>2018-05-22T18:14:52Z</updated><published>2018-05-22T18:14:52Z</published><content type="html">&lt;p&gt;[In case you aren&amp;#8217;t following the &lt;a href="https://blog.openshift.com/"&gt;OpenShift blog&lt;/a&gt;, I&amp;#8217;m cross posting &lt;a href="https://blog.openshift.com/using-ansible-galaxy-roles-in-ansible-playbook-bundles"&gt;my article&lt;/a&gt; here because I think it will be of interest to the Red Hat Developer commnity.]&lt;/p&gt; &lt;p&gt;The Open Service Broker API standard aims to standardize how services (cloud, third-party, on-premise, legacy, etc) are delivered to applications running on cloud platforms like OpenShift. This allows applications to consume services the exact same way no matter on which cloud platform they are deployed. The service broker pluggable architecture enables admins to add third-party brokers to the platform in order to make third-party and cloud services available to the application developers directly from the OpenShift service catalog. As an example &lt;a href="https://aws.amazon.com/partners/servicebroker/"&gt;AWS Service Broker&lt;/a&gt; created jointly by Amazon and Red Hat, &lt;a href="https://github.com/azure/open-service-broker-azure"&gt;Azure Service Broker&lt;/a&gt; created by Microsoft and &lt;a href="https://github.com/google/helm-broker"&gt;Helm Service Broker&lt;/a&gt; created by Google to allow consumption of AWS services, Azure services and Helm charts on Kubernetes and OpenShift. Furthermore, admins can &lt;a href="https://github.com/openshift/open-service-broker-sdk"&gt;create their own brokers&lt;/a&gt; in order to make custom services like provisioning an Oracle database on their internal Oracle RAC available to the developers through the service catalog.&lt;span id="more-496237"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-496277 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1.png 940w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1-300x144.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1-768x368.png 768w" sizes="(max-width: 940px) 100vw, 940px" /&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://automationbroker.io/"&gt;OpenShift Automation Broker&lt;/a&gt; is a service broker that is included in OpenShift out-of-the-box and leverages a lightweight, container-based application definition called an Ansible Playbook Bundle (APB) to automate service provisioning using Ansible. The playbooks can perform actions on OpenShift platform, as well as off platform such as provisioning a virtual machine on VMware and installing a database on it.&lt;/p&gt; &lt;h3&gt;Ansible Galaxy Roles&lt;/h3&gt; &lt;p&gt;The &lt;a href="https://docs.openshift.com/container-platform/3.9/apb_devel/writing/getting_started.html#apb-devel-writing-gs-creating"&gt;current structure of APBs&lt;/a&gt; requires all Ansible roles used in the playbooks to be available in the &lt;code&gt;roles&lt;/code&gt; directory, as Ansible normally demands it.&lt;/p&gt; &lt;pre&gt;myapb ├── apb.yml ├── Dockerfile ├── playbooks │ ├── deprovision.yml │ └── provision.yml └── roles ├── myrole1 └── myrole2 &lt;/pre&gt; &lt;p&gt;While that is useful, the power of Ansible lies in the large ecosystem of Ansible roles that are created by third-parties and the community which simplifies automating virtually anything. The expression “there is role for that!” is not far from reality in the Ansible world.&lt;/p&gt; &lt;p&gt;&lt;a href="https://galaxy.ansible.com"&gt;Ansible Galaxy&lt;/a&gt; is the hub for finding, reusing and sharing Ansible roles. Whenever one needs to create an Ansible role, the first step is usually to search on Ansible Galaxy for a role that already does the task needed.&lt;/p&gt; &lt;p&gt;Although on the roadmap, currently the Ansible Galaxy roles cannot be directly used in the APB playbooks unless they are already downloaded in the roles directory. Nevertheless, you can still use Ansible Galaxy roles in the APB through a few extra steps which are explained in the next section.&lt;/p&gt; &lt;h3&gt;Using Ansible Galaxy Roles in APBs&lt;/h3&gt; &lt;p&gt;The first step for using the Ansible Galaxy roles is to list those roles as dependencies in a &lt;code&gt;requirements.yml&lt;/code&gt; file in the root of the APB which is the standard way of describing the Ansible Galaxy roles in Ansible:&lt;/p&gt; &lt;pre&gt;- src: siamaksade.openshift_sonatype_nexus version: ocp-3.9 - src: siamaksade.openshift_gogs version: ocp-3.9 - src: siamaksade.openshift_eclipse_che version: ocp-3.9-1 &lt;/pre&gt; &lt;p&gt;The APB directory structure would look like the following:&lt;/p&gt; &lt;pre&gt;myapb/ ├── apb.yml ├── Dockerfile ├── requirements.yml ├── playbooks │ ├── deprovision.yml │ └── provision.yml └── roles ├── myrole1 └── myrole2 &lt;/pre&gt; &lt;p&gt;The above &lt;code&gt;requirements.yml&lt;/code&gt; file declares that this APB requires the &lt;code&gt;openshift_sonatype_nexus&lt;/code&gt;, &lt;code&gt;openshift_gogs&lt;/code&gt; and &lt;code&gt;openshift_eclipse_che&lt;/code&gt; roles from Ansible Galaxy.&lt;/p&gt; &lt;p&gt;Since Ansible Galaxy is not integrated into APBs yet, the next step is to add a few lines to the APB &lt;code&gt;Dockerfile&lt;/code&gt; to install the dependencies when the APB is being built and packaged:&lt;/p&gt; &lt;pre&gt;ADD requirements.yml /opt/apb/actions/requirements.yml RUN ansible-galaxy install -r /opt/apb/actions/requirements.yml &lt;/pre&gt; &lt;p&gt;Now you can use the declared roles in your APB action playbooks, for example in the &lt;code&gt;playbooks/provision.yml&lt;/code&gt; which is the playbook that runs when an APB is provisioned via the OpenShift service catalog:&lt;/p&gt; &lt;pre&gt; roles: - role: ansible.kubernetes-modules install_python_requirements: no - role: ansibleplaybookbundle.asb-modules - role: siamaksade.openshift_sonatype_nexus - role: siamaksade.openshift_gogs - role: siamaksade.openshift_eclipse_che &lt;/pre&gt; &lt;h3&gt;Further Integration With Ansible Galaxy&lt;/h3&gt; &lt;p&gt;In OpenShift 3.11, APBs will become a first class citizen of Ansible Galaxy and can be shared and reused conveniently the very same way that Ansible roles are shared and reused in Ansible Galaxy. The OpenShift Automation Broker will be able to discover APBs directly from Ansible Galaxy which means that a developer can publish their APB source code to Ansible Galaxy and the OpenShift Automation Broker will run the source directly on a special APB base image. This will allow developers to test their source code changes without the hassle of rebuilding the full APB container images each time they want to test it.&lt;/p&gt; &lt;p&gt;Furthermore, the integration with Ansible Galaxy would allow automatic resolution of the role dependencies based on the APB metadata and would allow using any role that is published to Ansible Galaxy and declared as a dependency in the metadata, directly in the playbooks.&lt;/p&gt; &lt;h3&gt;Summary&lt;/h3&gt; &lt;p&gt;APBs enable using Ansible playbooks for automating provisioning of services on OpenShift and also other platforms. Ansible Galaxy enriches the Ansible experience by providing a large set of pre-built roles that can be used to automate virtually anything. By creating a &lt;code&gt;requirements.yml&lt;/code&gt; file and modifying the &lt;code&gt;Dockerfile&lt;/code&gt; in an APB, one can take advantage of the Ansible Galaxy roles in authoring APBs and build complex automation flows.&lt;/p&gt; &lt;p&gt;Ansible Galaxy roles play an important role in APB authoring and therefore native Ansible Galaxy support in APBs is planned for future releases of OpenShift, possibly in OpenShift 3.11.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;title=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" data-a2a-url="https://developers.redhat.com/blog/2018/05/22/using-ansible-galaxy-roles-in-ansible-playbook-bundles/" data-a2a-title="Using Ansible Galaxy Roles in Ansible Playbook Bundles"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/22/using-ansible-galaxy-roles-in-ansible-playbook-bundles/"&gt;Using Ansible Galaxy Roles in Ansible Playbook Bundles&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KhvDAScZbGM" height="1" width="1" alt=""/&gt;</content><summary>[In case you aren’t following the OpenShift blog, I’m cross posting my article here because I think it will be of interest to the Red Hat Developer commnity.] The Open Service Broker API standard aims to standardize how services (cloud, third-party, on-premise, legacy, etc) are delivered to applications running on cloud platforms like OpenShift. This allows applications to consume services the exa...</summary><dc:creator>Siamak Sadeghianfar</dc:creator><dc:date>2018-05-22T18:14:52Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/22/using-ansible-galaxy-roles-in-ansible-playbook-bundles/</feedburner:origLink></entry></feed>
